{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joseph Rochelle\n",
    "## DSC 550 Data Mining\n",
    "## *Week 2: Text Classification with Machine learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "**1. You can find the dataset controversial-comments.jsonl for this exercise in the Weekly Resources: Week 2 Data Files.\n",
    "Pre-processing Text: For this part, you will start by reading the controversial-comments.jsonl file into a DataFrame. Then:**\n",
    "\n",
    "    A. Convert all text to lowercase letters.\n",
    "\n",
    "    B. Remove all punctuation from the text.\n",
    "\n",
    "    C. Remove stop words.\n",
    "\n",
    "    D. Apply NLTKâ€™s PorterStemmer.\n",
    "\n",
    "**2. Now that the data is pre-processed, you will apply three different techniques to get it into a usable form for model-building. Apply each of the following steps (individually) to the pre-processed data.**\n",
    "\n",
    "    A. Convert each text entry into a word-count vector (see sections 5.3 & 6.8 in the Machine Learning with Python Cookbook).\n",
    "\n",
    "    B. Convert each text entry into a part-of-speech tag vector (see section 6.7 in the Machine Learning with Python Cookbook).\n",
    "\n",
    "    C. Convert each entry into a term frequency-inverse document frequency (tfidf) vector (see section 6.9 in the Machine Learning with Python Cookbook).\n",
    "\n",
    "**Follow-Up Question:**\n",
    "\n",
    "For the three techniques in problem (2) above, give an example where each would be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries used\n",
    "#I did install jsonlines as well- !pip install jsonlines\n",
    "#Reminder for those using VPN to turn it off \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data file\n",
    "OG_datafile = pd.read_json('controversial-comments.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The file is so large dropped it to 20. Will increase later. \n",
    "Text_datafile = OG_datafile.sample(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1A: Lower Case of Txt:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to lowercase the words\n",
    "def lowercapitalizer(string: str) -> str:\n",
    "    return string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt= [lowercapitalizer(string) for string in Text_datafile['txt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1B: Removing punctuation of Txt:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loaidng library\n",
    "import unicodedata \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all puncutaiton\n",
    "#Creating a dictionary of puncutation \n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                           if unicodedata.category(chr(i)).startswith('P'))\n",
    "#Removing puncuation \n",
    "txt= [string.translate(punctuation) for string in Text_datafile['txt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=Text_datafile['txt'].str.replace(r'[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>23973</td>\n",
       "      <td>Operated might be a little strong owned for su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158673</td>\n",
       "      <td>gt want to be taxed\\n\\nThis seems to be the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632749</td>\n",
       "      <td>to all the rocknrollers and holy rollers the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>633692</td>\n",
       "      <td>If you read more of the article and look at th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642357</td>\n",
       "      <td>Sure I do poptart Sure I do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248611</td>\n",
       "      <td>removed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165550</td>\n",
       "      <td>deleted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329002</td>\n",
       "      <td>I live in Ex Yugoslavia and after fall of Comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176235</td>\n",
       "      <td>There is no reasoning with these people they a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362384</td>\n",
       "      <td>He began fucking up when he started fanning th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302357</td>\n",
       "      <td>Its amazing how there wasnt even an attempt to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95589</td>\n",
       "      <td>Now this is some good sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392835</td>\n",
       "      <td>Theyve got the legal system on their side I do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144718</td>\n",
       "      <td>We need CNN to comb through it first before de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150568</td>\n",
       "      <td>People still claiming that Clinton was more el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792805</td>\n",
       "      <td>Uh dipshits in Pennsylvania Ohio Michigan and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114183</td>\n",
       "      <td>Oh I get it now youre trolling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711368</td>\n",
       "      <td>Unfortunately this submission has been removed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161670</td>\n",
       "      <td>Im going to go out on a limb here folks and sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403778</td>\n",
       "      <td>We also dont know what Bill Clinton said to th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      txt\n",
       "23973   Operated might be a little strong owned for su...\n",
       "158673  gt want to be taxed\\n\\nThis seems to be the po...\n",
       "632749  to all the rocknrollers and holy rollers the c...\n",
       "633692  If you read more of the article and look at th...\n",
       "642357                        Sure I do poptart Sure I do\n",
       "248611                                            removed\n",
       "165550                                            deleted\n",
       "329002  I live in Ex Yugoslavia and after fall of Comm...\n",
       "176235  There is no reasoning with these people they a...\n",
       "362384  He began fucking up when he started fanning th...\n",
       "302357  Its amazing how there wasnt even an attempt to...\n",
       "95589                       Now this is some good sarcasm\n",
       "392835  Theyve got the legal system on their side I do...\n",
       "144718  We need CNN to comb through it first before de...\n",
       "150568  People still claiming that Clinton was more el...\n",
       "792805  Uh dipshits in Pennsylvania Ohio Michigan and ...\n",
       "114183                     Oh I get it now youre trolling\n",
       "711368  Unfortunately this submission has been removed...\n",
       "161670  Im going to go out on a limb here folks and sa...\n",
       "403778  We also dont know what Bill Clinton said to th..."
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt =pd.DataFrame(txt)\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tokenizing the words\n",
    "split_data =txt['txt'].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23973     [Operated, might, be, a, little, strong, owned...\n",
      "158673    [gt, want, to, be, taxed\\n\\nThis, seems, to, b...\n",
      "632749    [to, all, the, rocknrollers, and, holy, roller...\n",
      "633692    [If, you, read, more, of, the, article, and, l...\n",
      "642357                  [Sure, I, do, poptart, Sure, I, do]\n",
      "248611                                            [removed]\n",
      "165550                                            [deleted]\n",
      "329002    [I, live, in, Ex, Yugoslavia, and, after, fall...\n",
      "176235    [There, is, no, reasoning, with, these, people...\n",
      "362384    [He, began, fucking, up, when, he, started, fa...\n",
      "302357    [Its, amazing, how, there, wasnt, even, an, at...\n",
      "95589                  [Now, this, is, some, good, sarcasm]\n",
      "392835    [Theyve, got, the, legal, system, on, their, s...\n",
      "144718    [We, need, CNN, to, comb, through, it, first, ...\n",
      "150568    [People, still, claiming, that, Clinton, was, ...\n",
      "792805    [Uh, dipshits, in, Pennsylvania, Ohio, Michiga...\n",
      "114183               [Oh, I, get, it, now, youre, trolling]\n",
      "711368    [Unfortunately, this, submission, has, been, r...\n",
      "161670    [Im, going, to, go, out, on, a, limb, here, fo...\n",
      "403778    [We, also, dont, know, what, Bill, Clinton, sa...\n",
      "Name: txt, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1C: Removing stop words of Txt:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Loading Libary\n",
    "# Working on the stop words removal \n",
    "\n",
    "from nltk.corpus import stopwords #You have to download this file.\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loadstop words\n",
    "stop_words= set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =[word for word in txt if not word in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1D: Apply NLTK PorterStemmer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load library\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create stemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stem_data = data.apply(lambda x: [porter.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23973     [oper, might, be, a, littl, strong, own, for, ...\n",
       "158673    [gt, want, to, be, taxed\\n\\nthi, seem, to, be,...\n",
       "632749    [to, all, the, rocknrol, and, holi, roller, th...\n",
       "633692    [If, you, read, more, of, the, articl, and, lo...\n",
       "642357                  [sure, I, do, poptart, sure, I, do]\n",
       "248611                                              [remov]\n",
       "165550                                              [delet]\n",
       "329002    [I, live, in, Ex, yugoslavia, and, after, fall...\n",
       "176235    [there, is, no, reason, with, these, peopl, th...\n",
       "362384    [He, began, fuck, up, when, he, start, fan, th...\n",
       "302357    [it, amaz, how, there, wasnt, even, an, attemp...\n",
       "95589                   [now, thi, is, some, good, sarcasm]\n",
       "392835    [theyv, got, the, legal, system, on, their, si...\n",
       "144718    [We, need, cnn, to, comb, through, it, first, ...\n",
       "150568    [peopl, still, claim, that, clinton, wa, more,...\n",
       "792805    [Uh, dipshit, in, pennsylvania, ohio, michigan...\n",
       "114183                   [Oh, I, get, it, now, your, troll]\n",
       "711368    [unfortun, thi, submiss, ha, been, remov, per,...\n",
       "161670    [Im, go, to, go, out, on, a, limb, here, folk,...\n",
       "403778    [We, also, dont, know, what, bill, clinton, sa...\n",
       "Name: txt, dtype: object"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Operated is now oper \n",
    "Stem_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2A: Word Count Vector:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading library\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert into a string\n",
    "Practice_list= ''.join(str(v) for v in Stem_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert into a dict\n",
    "Practice_dict = {i: Practice_list[i] for i in range(0,len(Practice_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dict vector\n",
    "dictvectorizer = DictVectorizer(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert dict to feature matrix\n",
    "practice_text = dictvectorizer.fit_transform(Practice_dict)\n",
    "practice_array= np.array([Practice_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 363)\t2\n",
      "  (0, 319)\t1\n",
      "  (0, 54)\t17\n",
      "  (0, 293)\t4\n",
      "  (0, 479)\t1\n",
      "  (0, 369)\t6\n",
      "  (0, 204)\t20\n",
      "  (0, 488)\t4\n",
      "  (0, 82)\t7\n",
      "  (0, 540)\t3\n",
      "  (0, 265)\t21\n",
      "  (0, 243)\t2\n",
      "  (0, 505)\t58\n",
      "  (0, 235)\t2\n",
      "  (0, 273)\t2\n",
      "  (0, 355)\t44\n",
      "  (0, 179)\t1\n",
      "  (0, 234)\t1\n",
      "  (0, 414)\t1\n",
      "  (0, 392)\t1\n",
      "  (0, 201)\t1\n",
      "  (0, 305)\t1\n",
      "  (0, 67)\t3\n",
      "  (0, 130)\t2\n",
      "  (0, 212)\t4\n",
      "  :\t:\n",
      "  (0, 341)\t1\n",
      "  (0, 393)\t1\n",
      "  (0, 568)\t1\n",
      "  (0, 107)\t1\n",
      "  (0, 157)\t1\n",
      "  (0, 180)\t1\n",
      "  (0, 174)\t1\n",
      "  (0, 282)\t1\n",
      "  (0, 55)\t1\n",
      "  (0, 8)\t2\n",
      "  (0, 248)\t1\n",
      "  (0, 325)\t1\n",
      "  (0, 69)\t1\n",
      "  (0, 46)\t1\n",
      "  (0, 314)\t1\n",
      "  (0, 185)\t2\n",
      "  (0, 309)\t1\n",
      "  (0, 454)\t2\n",
      "  (0, 339)\t1\n",
      "  (0, 153)\t1\n",
      "  (0, 313)\t1\n",
      "  (0, 205)\t1\n",
      "  (0, 461)\t1\n",
      "  (0, 431)\t1\n",
      "  (0, 30)\t1\n"
     ]
    }
   ],
   "source": [
    "#Create bag of words feature matrix\n",
    "count= CountVectorizer()\n",
    "bag_of_words =count.fit_transform(practice_array)\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2B: Part of Speech:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rochjo1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load libraries \n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data =\"Chris loved outdoor running\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tagged = pos_tag(word_tokenize(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** I was not able to figure out the token data of 1D to this** The BOOk does not have it and peers are all scattered saying read 101-109 that i did for 30 mins. \n",
    "\n",
    "** I also don't feel confident i could get the TFIDF b/c once again there is missing content from the book to what you actually do. And, other resources were not great *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Follow-up Questions:**\n",
    "    \n",
    "--1) Counting the words in a vector can be useful to count the occurence of a word and the meaning. It allows multidimensional floating points and can be used to describe animals with different dimensions like animal, domesticated, pet, or fluffy\n",
    "\n",
    "--2) Part of speech tags are my favorite becuase you can use them to understand meaning from a sentence. If i was to use this in chat data, i would find all the verbs that would be used to describe something like the iPhone as an example. Then, find a word cloud that could use it for various things. Another thing i would try is adjetives towards any noun that could help desribe things. \n",
    "\n",
    "--3) Term frequency-inverse. The book does a great job of showing term frequency of economics showing up as an important word because the referese of document frequency would pull out words that are less meaningful like phone that could have a higher frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
