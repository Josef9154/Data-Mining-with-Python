{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joseph Rochelle\n",
    "## DSC 550 Data Mining\n",
    "## 9.3 Neural Network Classifiers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Neural Network Classifier with Scikit\n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "\n",
    "2. Neural Network Classifier with Keras\n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using Keras. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "\n",
    "3. Classifying Images\n",
    "\n",
    "In chapter 20 of the Machine Learning with Python Cookbook, implement the code found in section 20.15 classify MSINT images using a convolutional neural network. Report the accuracy of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df= pd.read_json(\"categorized-comments1.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>Barely better than Gabbert? He was significant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>No!! NOO!!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt\n",
       "0  sports  Barely better than Gabbert? He was significant...\n",
       "1  sports  Fuck the ducks and the Angels! But welcome to ...\n",
       "2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...\n",
       "3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)\n",
       "4  sports                                      No!! NOO!!!!!"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import unicodedata\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Create a dictionary of punctionuation characters\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "# For each string, remove any punctuation characters\n",
    "df['txt'] = [string.translate(punctuation) for string in df.txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>Barely better than Gabbert He was significantl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>Fuck the ducks and the Angels But welcome to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>Should have drafted more WRs\\n\\n Matt Millen p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>Donehttpsiimgurcom2YZ90pmjpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>No NOO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sports</td>\n",
       "      <td>Ding dong the Kaepers gone Yes Friday off to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sports</td>\n",
       "      <td>yup\\n\\nThat would be best case scenario Still ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sports</td>\n",
       "      <td>I think Larry Kruger made a good point on KNBR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sports</td>\n",
       "      <td>This is great to have two wellregarded RB coac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sports</td>\n",
       "      <td>79 next season confirmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sports</td>\n",
       "      <td>Familiarity with the system is why I have thos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat                                                txt\n",
       "0   sports  Barely better than Gabbert He was significantl...\n",
       "1   sports  Fuck the ducks and the Angels But welcome to a...\n",
       "2   sports  Should have drafted more WRs\\n\\n Matt Millen p...\n",
       "3   sports                       Donehttpsiimgurcom2YZ90pmjpg\n",
       "4   sports                                             No NOO\n",
       "5   sports  Ding dong the Kaepers gone Yes Friday off to a...\n",
       "6   sports  yup\\n\\nThat would be best case scenario Still ...\n",
       "7   sports  I think Larry Kruger made a good point on KNBR...\n",
       "8   sports  This is great to have two wellregarded RB coac...\n",
       "9   sports                           79 next season confirmed\n",
       "10  sports  Familiarity with the system is why I have thos..."
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words\n",
    "\n",
    "#load library\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load stop words\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# tokenize the words\n",
    "df['txt'] = [word_tokenize(string) for string in df.txt]\n",
    "\n",
    "# remove stop words\n",
    "df['txt'] = df['txt'].apply(lambda x: [item for item in x if item not in stopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>[Barely, better, Gabbert, He, significantly, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>[Fuck, ducks, Angels, But, welcome, new, niner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>[Should, drafted, WRs, Matt, Millen, probably]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[Donehttpsiimgurcom2YZ90pmjpg]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>[No, NOO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sports</td>\n",
       "      <td>[Ding, dong, Kaepers, gone, Yes, Friday, good,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sports</td>\n",
       "      <td>[yup, That, would, best, case, scenario, Still...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sports</td>\n",
       "      <td>[I, think, Larry, Kruger, made, good, point, K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sports</td>\n",
       "      <td>[This, great, two, wellregarded, RB, coaches, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sports</td>\n",
       "      <td>[79, next, season, confirmed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sports</td>\n",
       "      <td>[Familiarity, system, I, guys, higher]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat                                                txt\n",
       "0   sports  [Barely, better, Gabbert, He, significantly, b...\n",
       "1   sports  [Fuck, ducks, Angels, But, welcome, new, niner...\n",
       "2   sports     [Should, drafted, WRs, Matt, Millen, probably]\n",
       "3   sports                     [Donehttpsiimgurcom2YZ90pmjpg]\n",
       "4   sports                                          [No, NOO]\n",
       "5   sports  [Ding, dong, Kaepers, gone, Yes, Friday, good,...\n",
       "6   sports  [yup, That, would, best, case, scenario, Still...\n",
       "7   sports  [I, think, Larry, Kruger, made, good, point, K...\n",
       "8   sports  [This, great, two, wellregarded, RB, coaches, ...\n",
       "9   sports                      [79, next, season, confirmed]\n",
       "10  sports             [Familiarity, system, I, guys, higher]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(606475, 2)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dramatically reduced the DF\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming of the words\n",
    "\n",
    "# load library\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# create stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Apply stemmer\n",
    "\n",
    "df['txt'] = df['txt'].apply(lambda x: [porter.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>[bare, better, gabbert, He, significantli, bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>[fuck, duck, angel, but, welcom, new, niner, fan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>[should, draft, wr, matt, millen, probabl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[donehttpsiimgurcom2yz90pmjpg]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>[No, noo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sports</td>\n",
       "      <td>[ding, dong, kaeper, gone, ye, friday, good, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sports</td>\n",
       "      <td>[yup, that, would, best, case, scenario, still...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sports</td>\n",
       "      <td>[I, think, larri, kruger, made, good, point, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sports</td>\n",
       "      <td>[thi, great, two, wellregard, RB, coach, team,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sports</td>\n",
       "      <td>[79, next, season, confirm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sports</td>\n",
       "      <td>[familiar, system, I, guy, higher]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat                                                txt\n",
       "0   sports  [bare, better, gabbert, He, significantli, bet...\n",
       "1   sports  [fuck, duck, angel, but, welcom, new, niner, fan]\n",
       "2   sports         [should, draft, wr, matt, millen, probabl]\n",
       "3   sports                     [donehttpsiimgurcom2yz90pmjpg]\n",
       "4   sports                                          [No, noo]\n",
       "5   sports  [ding, dong, kaeper, gone, ye, friday, good, s...\n",
       "6   sports  [yup, that, would, best, case, scenario, still...\n",
       "7   sports  [I, think, larri, kruger, made, good, point, k...\n",
       "8   sports  [thi, great, two, wellregard, RB, coach, team,...\n",
       "9   sports                        [79, next, season, confirm]\n",
       "10  sports                 [familiar, system, I, guy, higher]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of Speech\n",
    "#Libraries\n",
    "from nltk import pos_tag\n",
    "#from nltk import word_tokenize\n",
    "\n",
    "# Use pre-trained part of speech tagger\n",
    "textTagged = df['txt'].apply(lambda x: [pos_tag(x)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [[(bare, NN), (better, RBR), (gabbert, NN), (H...\n",
       "1         [[(fuck, JJ), (duck, NN), (angel, NN), (but, C...\n",
       "2         [[(should, MD), (draft, VB), (wr, NN), (matt, ...\n",
       "3                    [[(donehttpsiimgurcom2yz90pmjpg, NN)]]\n",
       "4                                   [[(No, DT), (noo, NN)]]\n",
       "                                ...                        \n",
       "606470    [[(gtani, NN), (chanc, NN), (instal, JJ), (ent...\n",
       "606471    [[(No, DT), (it, PRP), (probabl, VBZ), (happen...\n",
       "606472    [[(I, PRP), (think, VBP), (disappoint, NN), (c...\n",
       "606473    [[(dishonor, NN), (12, CD), (look, NN), (like,...\n",
       "606474                                      [[(remov, NN)]]\n",
       "Name: txt, Length: 606475, dtype: object"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textTagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>[bare, better, gabbert, He, significantli, bet...</td>\n",
       "      <td>[[(bare, NN), (better, RBR), (gabbert, NN), (H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>[fuck, duck, angel, but, welcom, new, niner, fan]</td>\n",
       "      <td>[[(fuck, JJ), (duck, NN), (angel, NN), (but, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>[should, draft, wr, matt, millen, probabl]</td>\n",
       "      <td>[[(should, MD), (draft, VB), (wr, NN), (matt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[donehttpsiimgurcom2yz90pmjpg]</td>\n",
       "      <td>[[(donehttpsiimgurcom2yz90pmjpg, NN)]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>[No, noo]</td>\n",
       "      <td>[[(No, DT), (noo, NN)]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt  \\\n",
       "0  sports  [bare, better, gabbert, He, significantli, bet...   \n",
       "1  sports  [fuck, duck, angel, but, welcom, new, niner, fan]   \n",
       "2  sports         [should, draft, wr, matt, millen, probabl]   \n",
       "3  sports                     [donehttpsiimgurcom2yz90pmjpg]   \n",
       "4  sports                                          [No, noo]   \n",
       "\n",
       "                                                 pos  \n",
       "0  [[(bare, NN), (better, RBR), (gabbert, NN), (H...  \n",
       "1  [[(fuck, JJ), (duck, NN), (angel, NN), (but, C...  \n",
       "2  [[(should, MD), (draft, VB), (wr, NN), (matt, ...  \n",
       "3             [[(donehttpsiimgurcom2yz90pmjpg, NN)]]  \n",
       "4                            [[(No, DT), (noo, NN)]]  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part of speech added to data frame as a tupple\n",
    "#Added to last DF\n",
    "df['pos'] = textTagged\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# export df to csv only to speed up the model since we are using high dimensionality \n",
    "# After text cleaned, doing a flat file as the speed of the entire exercise was taking about an hour. \n",
    "\n",
    "df.to_csv('categComments.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = 0.0016  # .16% of the lines\n",
    "# if random from [0,1] interval is greater than 0.0016 the row will be skipped\n",
    "df = pd.read_csv('categComments.csv', \n",
    "         skiprows=lambda i: i>0 and random.random() > p\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now target Variables for Neural Network Classifiers \n",
    "#Cleaning df\n",
    "X = df.drop(['cat', 'pos'], axis = 1) \n",
    "y = df['cat']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X for feature\n",
    "# y for Variable\n",
    "X = df.txt \n",
    "y = df.cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ['ani', 'thought', 'anyon', 'besid', 'pepper']\n",
       "1         ['charl', 'want', 'go', 'contend', 'anyway']\n",
       "2               ['you', 'didnt', 'answer', 'question']\n",
       "3    ['gtwin', 'class', 'fuck', 'let', 'win', 'yeah...\n",
       "4    ['worst', 'case', 'scenario', 'play', 'somewhe...\n",
       "Name: txt, dtype: object"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    sports\n",
       "1    sports\n",
       "2    sports\n",
       "3    sports\n",
       "4    sports\n",
       "Name: cat, dtype: object"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "#ordinal encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "y = label_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         ['ani', 'thought', 'anyon', 'besid', 'pepper']\n",
      "1           ['charl', 'want', 'go', 'contend', 'anyway']\n",
      "2                 ['you', 'didnt', 'answer', 'question']\n",
      "3      ['gtwin', 'class', 'fuck', 'let', 'win', 'yeah...\n",
      "4      ['worst', 'case', 'scenario', 'play', 'somewhe...\n",
      "                             ...                        \n",
      "956    ['the', 'plant', 'move', 'prison', 'escap', 'd...\n",
      "957    ['I', 'hadnt', 'thought', 'use', 'filter', 'tl...\n",
      "958    ['I', 'download', 'updat', '8', 'time', '3', '...\n",
      "959                           ['transistor', 'headphon']\n",
      "960                           ['infam', 'second', 'son']\n",
      "Name: txt, Length: 961, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(961,)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y.reshape((961, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Network Classifier with Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a function to train the neural network classifier model\n",
    "\n",
    "def train_model(model,X,y, saveto=None, cv=12):\n",
    "    \"\"\"\n",
    "    Trains model from corpus at specified path; constructing cross-validation\n",
    "    scores using the cv parameter, then fitting the model on the full data and\n",
    "    writing it to disk at the saveto path if specified. Returns the scores.\n",
    "    \"\"\"\n",
    "    # Load the corpus data and labels for classification\n",
    "#     corpus = PickledCorpusReader(path)\n",
    "    # corpus is df\n",
    "    X = list(X)\n",
    "    y = list(y)\n",
    "    scoring = {'accuracy': 'accuracy',\n",
    "           'precision': 'precision_macro',\n",
    "           'recall': 'recall_macro',\n",
    "              'f1': 'f1_macro'}\n",
    "\n",
    "    # Compute cross validation scores\n",
    "    scores = cross_validate(model, X, y, cv=cv, scoring = scoring)\n",
    "\n",
    "    # Fit the model on entire data set\n",
    "    model1 = model.fit(X, y)\n",
    "\n",
    "    # Write to disk if specified\n",
    "    if saveto:\n",
    "        joblib.dump(model, saveto)\n",
    "\n",
    "    # Return fitted model \n",
    "    \n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a pipeline to create the model for training\n",
    "\n",
    "# import libaries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# build pipeline\n",
    "classifier = Pipeline([\n",
    "    ('tdif', TfidfVectorizer()),\n",
    "    ('ann', MLPClassifier(hidden_layer_sizes=(100, ), verbose=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.11584235\n",
      "Iteration 2, loss = 1.05868134\n",
      "Iteration 3, loss = 1.00627971\n",
      "Iteration 4, loss = 0.95161347\n",
      "Iteration 5, loss = 0.89384857\n",
      "Iteration 6, loss = 0.83259550\n",
      "Iteration 7, loss = 0.77249147\n",
      "Iteration 8, loss = 0.71362439\n",
      "Iteration 9, loss = 0.66064738\n",
      "Iteration 10, loss = 0.61390352\n",
      "Iteration 11, loss = 0.57273228\n",
      "Iteration 12, loss = 0.53527516\n",
      "Iteration 13, loss = 0.50064841\n",
      "Iteration 14, loss = 0.46815228\n",
      "Iteration 15, loss = 0.43615225\n",
      "Iteration 16, loss = 0.40561882\n",
      "Iteration 17, loss = 0.37614668\n",
      "Iteration 18, loss = 0.34863350\n",
      "Iteration 19, loss = 0.32251924\n",
      "Iteration 20, loss = 0.29766939\n",
      "Iteration 21, loss = 0.27493711\n",
      "Iteration 22, loss = 0.25382639\n",
      "Iteration 23, loss = 0.23464741\n",
      "Iteration 24, loss = 0.21713290\n",
      "Iteration 25, loss = 0.20146052\n",
      "Iteration 26, loss = 0.18683722\n",
      "Iteration 27, loss = 0.17417902\n",
      "Iteration 28, loss = 0.16228786\n",
      "Iteration 29, loss = 0.15175827\n",
      "Iteration 30, loss = 0.14242730\n",
      "Iteration 31, loss = 0.13397976\n",
      "Iteration 32, loss = 0.12610604\n",
      "Iteration 33, loss = 0.11917953\n",
      "Iteration 34, loss = 0.11288594\n",
      "Iteration 35, loss = 0.10718017\n",
      "Iteration 36, loss = 0.10200866\n",
      "Iteration 37, loss = 0.09742527\n",
      "Iteration 38, loss = 0.09319315\n",
      "Iteration 39, loss = 0.08938169\n",
      "Iteration 40, loss = 0.08595734\n",
      "Iteration 41, loss = 0.08286687\n",
      "Iteration 42, loss = 0.07990297\n",
      "Iteration 43, loss = 0.07722614\n",
      "Iteration 44, loss = 0.07497856\n",
      "Iteration 45, loss = 0.07261084\n",
      "Iteration 46, loss = 0.07057494\n",
      "Iteration 47, loss = 0.06876720\n",
      "Iteration 48, loss = 0.06706377\n",
      "Iteration 49, loss = 0.06540949\n",
      "Iteration 50, loss = 0.06398654\n",
      "Iteration 51, loss = 0.06251946\n",
      "Iteration 52, loss = 0.06121579\n",
      "Iteration 53, loss = 0.06007519\n",
      "Iteration 54, loss = 0.05909528\n",
      "Iteration 55, loss = 0.05794653\n",
      "Iteration 56, loss = 0.05694683\n",
      "Iteration 57, loss = 0.05608184\n",
      "Iteration 58, loss = 0.05526294\n",
      "Iteration 59, loss = 0.05464197\n",
      "Iteration 60, loss = 0.05376935\n",
      "Iteration 61, loss = 0.05308592\n",
      "Iteration 62, loss = 0.05232172\n",
      "Iteration 63, loss = 0.05172073\n",
      "Iteration 64, loss = 0.05124313\n",
      "Iteration 65, loss = 0.05064345\n",
      "Iteration 66, loss = 0.05009021\n",
      "Iteration 67, loss = 0.04954540\n",
      "Iteration 68, loss = 0.04909579\n",
      "Iteration 69, loss = 0.04860146\n",
      "Iteration 70, loss = 0.04834936\n",
      "Iteration 71, loss = 0.04788258\n",
      "Iteration 72, loss = 0.04745112\n",
      "Iteration 73, loss = 0.04712277\n",
      "Iteration 74, loss = 0.04671922\n",
      "Iteration 75, loss = 0.04636513\n",
      "Iteration 76, loss = 0.04599281\n",
      "Iteration 77, loss = 0.04574968\n",
      "Iteration 78, loss = 0.04571202\n",
      "Iteration 79, loss = 0.04533753\n",
      "Iteration 80, loss = 0.04498299\n",
      "Iteration 81, loss = 0.04479524\n",
      "Iteration 82, loss = 0.04436405\n",
      "Iteration 83, loss = 0.04423351\n",
      "Iteration 84, loss = 0.04401545\n",
      "Iteration 85, loss = 0.04369384\n",
      "Iteration 86, loss = 0.04352074\n",
      "Iteration 87, loss = 0.04330423\n",
      "Iteration 88, loss = 0.04306622\n",
      "Iteration 89, loss = 0.04288857\n",
      "Iteration 90, loss = 0.04266421\n",
      "Iteration 91, loss = 0.04248267\n",
      "Iteration 92, loss = 0.04230748\n",
      "Iteration 93, loss = 0.04220704\n",
      "Iteration 94, loss = 0.04207951\n",
      "Iteration 95, loss = 0.04187253\n",
      "Iteration 96, loss = 0.04194705\n",
      "Iteration 97, loss = 0.04151345\n",
      "Iteration 98, loss = 0.04135699\n",
      "Iteration 99, loss = 0.04154301\n",
      "Iteration 100, loss = 0.04160475\n",
      "Iteration 101, loss = 0.04141278\n",
      "Iteration 102, loss = 0.04104193\n",
      "Iteration 103, loss = 0.04089106\n",
      "Iteration 104, loss = 0.04070087\n",
      "Iteration 105, loss = 0.04058146\n",
      "Iteration 106, loss = 0.04052948\n",
      "Iteration 107, loss = 0.04052595\n",
      "Iteration 108, loss = 0.04030437\n",
      "Iteration 109, loss = 0.04017282\n",
      "Iteration 110, loss = 0.04001818\n",
      "Iteration 111, loss = 0.03991489\n",
      "Iteration 112, loss = 0.03978677\n",
      "Iteration 113, loss = 0.03984464\n",
      "Iteration 114, loss = 0.03972185\n",
      "Iteration 115, loss = 0.03960059\n",
      "Iteration 116, loss = 0.03942303\n",
      "Iteration 117, loss = 0.03941314\n",
      "Iteration 118, loss = 0.03941102\n",
      "Iteration 119, loss = 0.03933907\n",
      "Iteration 120, loss = 0.03929041\n",
      "Iteration 121, loss = 0.03914837\n",
      "Iteration 122, loss = 0.03898393\n",
      "Iteration 123, loss = 0.03912978\n",
      "Iteration 124, loss = 0.03889830\n",
      "Iteration 125, loss = 0.03893962\n",
      "Iteration 126, loss = 0.03876883\n",
      "Iteration 127, loss = 0.03877804\n",
      "Iteration 128, loss = 0.03859632\n",
      "Iteration 129, loss = 0.03852926\n",
      "Iteration 130, loss = 0.03845885\n",
      "Iteration 131, loss = 0.03843290\n",
      "Iteration 132, loss = 0.03842767\n",
      "Iteration 133, loss = 0.03828538\n",
      "Iteration 134, loss = 0.03831222\n",
      "Iteration 135, loss = 0.03818080\n",
      "Iteration 136, loss = 0.03809202\n",
      "Iteration 137, loss = 0.03821531\n",
      "Iteration 138, loss = 0.03813108\n",
      "Iteration 139, loss = 0.03811183\n",
      "Iteration 140, loss = 0.03811039\n",
      "Iteration 141, loss = 0.03794218\n",
      "Iteration 142, loss = 0.03781135\n",
      "Iteration 143, loss = 0.03778531\n",
      "Iteration 144, loss = 0.03778300\n",
      "Iteration 145, loss = 0.03773071\n",
      "Iteration 146, loss = 0.03764123\n",
      "Iteration 147, loss = 0.03754950\n",
      "Iteration 148, loss = 0.03751961\n",
      "Iteration 149, loss = 0.03751781\n",
      "Iteration 150, loss = 0.03744286\n",
      "Iteration 151, loss = 0.03737490\n",
      "Iteration 152, loss = 0.03736652\n",
      "Iteration 153, loss = 0.03738857\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06829845\n",
      "Iteration 2, loss = 1.01888066\n",
      "Iteration 3, loss = 0.97301821\n",
      "Iteration 4, loss = 0.92638407\n",
      "Iteration 5, loss = 0.87774924\n",
      "Iteration 6, loss = 0.82588105\n",
      "Iteration 7, loss = 0.77269869\n",
      "Iteration 8, loss = 0.71883290\n",
      "Iteration 9, loss = 0.66921954\n",
      "Iteration 10, loss = 0.62322346\n",
      "Iteration 11, loss = 0.58035596\n",
      "Iteration 12, loss = 0.54133618\n",
      "Iteration 13, loss = 0.50419035\n",
      "Iteration 14, loss = 0.46845755\n",
      "Iteration 15, loss = 0.43489137\n",
      "Iteration 16, loss = 0.40214211\n",
      "Iteration 17, loss = 0.37109715\n",
      "Iteration 18, loss = 0.34180846\n",
      "Iteration 19, loss = 0.31445672\n",
      "Iteration 20, loss = 0.28942530\n",
      "Iteration 21, loss = 0.26641620\n",
      "Iteration 22, loss = 0.24632128\n",
      "Iteration 23, loss = 0.22731606\n",
      "Iteration 24, loss = 0.21072672\n",
      "Iteration 25, loss = 0.19581427\n",
      "Iteration 26, loss = 0.18192505\n",
      "Iteration 27, loss = 0.16976321\n",
      "Iteration 28, loss = 0.15863510\n",
      "Iteration 29, loss = 0.14859397\n",
      "Iteration 30, loss = 0.13940010\n",
      "Iteration 31, loss = 0.13124318\n",
      "Iteration 32, loss = 0.12369487\n",
      "Iteration 33, loss = 0.11678615\n",
      "Iteration 34, loss = 0.11061270\n",
      "Iteration 35, loss = 0.10472857\n",
      "Iteration 36, loss = 0.09956987\n",
      "Iteration 37, loss = 0.09494578\n",
      "Iteration 38, loss = 0.09047417\n",
      "Iteration 39, loss = 0.08681741\n",
      "Iteration 40, loss = 0.08316439\n",
      "Iteration 41, loss = 0.07996790\n",
      "Iteration 42, loss = 0.07699860\n",
      "Iteration 43, loss = 0.07430512\n",
      "Iteration 44, loss = 0.07198549\n",
      "Iteration 45, loss = 0.06968005\n",
      "Iteration 46, loss = 0.06771252\n",
      "Iteration 47, loss = 0.06578502\n",
      "Iteration 48, loss = 0.06414441\n",
      "Iteration 49, loss = 0.06250519\n",
      "Iteration 50, loss = 0.06109615\n",
      "Iteration 51, loss = 0.05967901\n",
      "Iteration 52, loss = 0.05839693\n",
      "Iteration 53, loss = 0.05728641\n",
      "Iteration 54, loss = 0.05612106\n",
      "Iteration 55, loss = 0.05523850\n",
      "Iteration 56, loss = 0.05435545\n",
      "Iteration 57, loss = 0.05342788\n",
      "Iteration 58, loss = 0.05255644\n",
      "Iteration 59, loss = 0.05176567\n",
      "Iteration 60, loss = 0.05107462\n",
      "Iteration 61, loss = 0.05044398\n",
      "Iteration 62, loss = 0.04976843\n",
      "Iteration 63, loss = 0.04910760\n",
      "Iteration 64, loss = 0.04860583\n",
      "Iteration 65, loss = 0.04818081\n",
      "Iteration 66, loss = 0.04760468\n",
      "Iteration 67, loss = 0.04712230\n",
      "Iteration 68, loss = 0.04666220\n",
      "Iteration 69, loss = 0.04632361\n",
      "Iteration 70, loss = 0.04583506\n",
      "Iteration 71, loss = 0.04547988\n",
      "Iteration 72, loss = 0.04511378\n",
      "Iteration 73, loss = 0.04478054\n",
      "Iteration 74, loss = 0.04438584\n",
      "Iteration 75, loss = 0.04411969\n",
      "Iteration 76, loss = 0.04376608\n",
      "Iteration 77, loss = 0.04356158\n",
      "Iteration 78, loss = 0.04326678\n",
      "Iteration 79, loss = 0.04300162\n",
      "Iteration 80, loss = 0.04275924\n",
      "Iteration 81, loss = 0.04250352\n",
      "Iteration 82, loss = 0.04228455\n",
      "Iteration 83, loss = 0.04200922\n",
      "Iteration 84, loss = 0.04195220\n",
      "Iteration 85, loss = 0.04182410\n",
      "Iteration 86, loss = 0.04147151\n",
      "Iteration 87, loss = 0.04127844\n",
      "Iteration 88, loss = 0.04108470\n",
      "Iteration 89, loss = 0.04096167\n",
      "Iteration 90, loss = 0.04075848\n",
      "Iteration 91, loss = 0.04064404\n",
      "Iteration 92, loss = 0.04046995\n",
      "Iteration 93, loss = 0.04029445\n",
      "Iteration 94, loss = 0.04032111\n",
      "Iteration 95, loss = 0.04016746\n",
      "Iteration 96, loss = 0.03991842\n",
      "Iteration 97, loss = 0.03978553\n",
      "Iteration 98, loss = 0.03972203\n",
      "Iteration 99, loss = 0.03960751\n",
      "Iteration 100, loss = 0.03949302\n",
      "Iteration 101, loss = 0.03934356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 102, loss = 0.03920528\n",
      "Iteration 103, loss = 0.03908550\n",
      "Iteration 104, loss = 0.03902338\n",
      "Iteration 105, loss = 0.03888136\n",
      "Iteration 106, loss = 0.03892057\n",
      "Iteration 107, loss = 0.03875345\n",
      "Iteration 108, loss = 0.03864179\n",
      "Iteration 109, loss = 0.03849872\n",
      "Iteration 110, loss = 0.03839339\n",
      "Iteration 111, loss = 0.03833948\n",
      "Iteration 112, loss = 0.03817829\n",
      "Iteration 113, loss = 0.03823023\n",
      "Iteration 114, loss = 0.03829077\n",
      "Iteration 115, loss = 0.03808740\n",
      "Iteration 116, loss = 0.03819017\n",
      "Iteration 117, loss = 0.03803592\n",
      "Iteration 118, loss = 0.03791802\n",
      "Iteration 119, loss = 0.03788354\n",
      "Iteration 120, loss = 0.03774410\n",
      "Iteration 121, loss = 0.03763897\n",
      "Iteration 122, loss = 0.03761880\n",
      "Iteration 123, loss = 0.03751225\n",
      "Iteration 124, loss = 0.03748118\n",
      "Iteration 125, loss = 0.03747760\n",
      "Iteration 126, loss = 0.03734385\n",
      "Iteration 127, loss = 0.03759785\n",
      "Iteration 128, loss = 0.03736398\n",
      "Iteration 129, loss = 0.03717578\n",
      "Iteration 130, loss = 0.03714210\n",
      "Iteration 131, loss = 0.03707824\n",
      "Iteration 132, loss = 0.03706886\n",
      "Iteration 133, loss = 0.03699384\n",
      "Iteration 134, loss = 0.03700616\n",
      "Iteration 135, loss = 0.03682164\n",
      "Iteration 136, loss = 0.03693644\n",
      "Iteration 137, loss = 0.03685866\n",
      "Iteration 138, loss = 0.03677115\n",
      "Iteration 139, loss = 0.03690142\n",
      "Iteration 140, loss = 0.03673696\n",
      "Iteration 141, loss = 0.03673910\n",
      "Iteration 142, loss = 0.03672271\n",
      "Iteration 143, loss = 0.03662278\n",
      "Iteration 144, loss = 0.03660788\n",
      "Iteration 145, loss = 0.03657398\n",
      "Iteration 146, loss = 0.03644168\n",
      "Iteration 147, loss = 0.03643384\n",
      "Iteration 148, loss = 0.03660460\n",
      "Iteration 149, loss = 0.03644033\n",
      "Iteration 150, loss = 0.03641707\n",
      "Iteration 151, loss = 0.03637330\n",
      "Iteration 152, loss = 0.03637539\n",
      "Iteration 153, loss = 0.03635717\n",
      "Iteration 154, loss = 0.03633143\n",
      "Iteration 155, loss = 0.03617374\n",
      "Iteration 156, loss = 0.03619057\n",
      "Iteration 157, loss = 0.03607582\n",
      "Iteration 158, loss = 0.03610184\n",
      "Iteration 159, loss = 0.03617346\n",
      "Iteration 160, loss = 0.03597916\n",
      "Iteration 161, loss = 0.03604097\n",
      "Iteration 162, loss = 0.03591619\n",
      "Iteration 163, loss = 0.03617600\n",
      "Iteration 164, loss = 0.03584623\n",
      "Iteration 165, loss = 0.03580674\n",
      "Iteration 166, loss = 0.03582175\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.02459101\n",
      "Iteration 2, loss = 0.96037550\n",
      "Iteration 3, loss = 0.90330325\n",
      "Iteration 4, loss = 0.84721134\n",
      "Iteration 5, loss = 0.79005983\n",
      "Iteration 6, loss = 0.73536904\n",
      "Iteration 7, loss = 0.68447892\n",
      "Iteration 8, loss = 0.63681303\n",
      "Iteration 9, loss = 0.59424659\n",
      "Iteration 10, loss = 0.55517632\n",
      "Iteration 11, loss = 0.51893188\n",
      "Iteration 12, loss = 0.48360663\n",
      "Iteration 13, loss = 0.45010530\n",
      "Iteration 14, loss = 0.41669478\n",
      "Iteration 15, loss = 0.38572659\n",
      "Iteration 16, loss = 0.35531880\n",
      "Iteration 17, loss = 0.32796230\n",
      "Iteration 18, loss = 0.30130440\n",
      "Iteration 19, loss = 0.27721146\n",
      "Iteration 20, loss = 0.25532474\n",
      "Iteration 21, loss = 0.23589724\n",
      "Iteration 22, loss = 0.21759600\n",
      "Iteration 23, loss = 0.20150757\n",
      "Iteration 24, loss = 0.18690586\n",
      "Iteration 25, loss = 0.17389573\n",
      "Iteration 26, loss = 0.16202862\n",
      "Iteration 27, loss = 0.15124030\n",
      "Iteration 28, loss = 0.14176819\n",
      "Iteration 29, loss = 0.13284391\n",
      "Iteration 30, loss = 0.12478873\n",
      "Iteration 31, loss = 0.11759973\n",
      "Iteration 32, loss = 0.11099612\n",
      "Iteration 33, loss = 0.10481220\n",
      "Iteration 34, loss = 0.09933673\n",
      "Iteration 35, loss = 0.09447930\n",
      "Iteration 36, loss = 0.08976696\n",
      "Iteration 37, loss = 0.08573936\n",
      "Iteration 38, loss = 0.08201487\n",
      "Iteration 39, loss = 0.07850083\n",
      "Iteration 40, loss = 0.07541958\n",
      "Iteration 41, loss = 0.07262372\n",
      "Iteration 42, loss = 0.07004230\n",
      "Iteration 43, loss = 0.06765873\n",
      "Iteration 44, loss = 0.06548723\n",
      "Iteration 45, loss = 0.06350936\n",
      "Iteration 46, loss = 0.06169098\n",
      "Iteration 47, loss = 0.06004663\n",
      "Iteration 48, loss = 0.05835430\n",
      "Iteration 49, loss = 0.05697758\n",
      "Iteration 50, loss = 0.05564194\n",
      "Iteration 51, loss = 0.05450819\n",
      "Iteration 52, loss = 0.05315838\n",
      "Iteration 53, loss = 0.05206562\n",
      "Iteration 54, loss = 0.05123966\n",
      "Iteration 55, loss = 0.05028953\n",
      "Iteration 56, loss = 0.04943401\n",
      "Iteration 57, loss = 0.04867492\n",
      "Iteration 58, loss = 0.04777212\n",
      "Iteration 59, loss = 0.04705566\n",
      "Iteration 60, loss = 0.04636692\n",
      "Iteration 61, loss = 0.04568814\n",
      "Iteration 62, loss = 0.04511538\n",
      "Iteration 63, loss = 0.04451807\n",
      "Iteration 64, loss = 0.04404771\n",
      "Iteration 65, loss = 0.04350188\n",
      "Iteration 66, loss = 0.04300068\n",
      "Iteration 67, loss = 0.04268312\n",
      "Iteration 68, loss = 0.04207438\n",
      "Iteration 69, loss = 0.04173943\n",
      "Iteration 70, loss = 0.04132381\n",
      "Iteration 71, loss = 0.04090848\n",
      "Iteration 72, loss = 0.04054496\n",
      "Iteration 73, loss = 0.04037654\n",
      "Iteration 74, loss = 0.03994928\n",
      "Iteration 75, loss = 0.03965841\n",
      "Iteration 76, loss = 0.03936749\n",
      "Iteration 77, loss = 0.03905651\n",
      "Iteration 78, loss = 0.03876112\n",
      "Iteration 79, loss = 0.03848674\n",
      "Iteration 80, loss = 0.03829035\n",
      "Iteration 81, loss = 0.03798561\n",
      "Iteration 82, loss = 0.03796535\n",
      "Iteration 83, loss = 0.03761389\n",
      "Iteration 84, loss = 0.03739198\n",
      "Iteration 85, loss = 0.03715706\n",
      "Iteration 86, loss = 0.03698843\n",
      "Iteration 87, loss = 0.03676483\n",
      "Iteration 88, loss = 0.03654950\n",
      "Iteration 89, loss = 0.03636181\n",
      "Iteration 90, loss = 0.03623067\n",
      "Iteration 91, loss = 0.03614009\n",
      "Iteration 92, loss = 0.03591772\n",
      "Iteration 93, loss = 0.03582296\n",
      "Iteration 94, loss = 0.03561763\n",
      "Iteration 95, loss = 0.03560277\n",
      "Iteration 96, loss = 0.03534459\n",
      "Iteration 97, loss = 0.03520385\n",
      "Iteration 98, loss = 0.03508115\n",
      "Iteration 99, loss = 0.03500021\n",
      "Iteration 100, loss = 0.03480701\n",
      "Iteration 101, loss = 0.03470516\n",
      "Iteration 102, loss = 0.03477077\n",
      "Iteration 103, loss = 0.03440733\n",
      "Iteration 104, loss = 0.03419439\n",
      "Iteration 105, loss = 0.03448246\n",
      "Iteration 106, loss = 0.03420451\n",
      "Iteration 107, loss = 0.03404333\n",
      "Iteration 108, loss = 0.03397170\n",
      "Iteration 109, loss = 0.03390768\n",
      "Iteration 110, loss = 0.03377584\n",
      "Iteration 111, loss = 0.03368065\n",
      "Iteration 112, loss = 0.03354033\n",
      "Iteration 113, loss = 0.03343851\n",
      "Iteration 114, loss = 0.03337022\n",
      "Iteration 115, loss = 0.03326383\n",
      "Iteration 116, loss = 0.03316603\n",
      "Iteration 117, loss = 0.03306604\n",
      "Iteration 118, loss = 0.03325709\n",
      "Iteration 119, loss = 0.03304525\n",
      "Iteration 120, loss = 0.03298086\n",
      "Iteration 121, loss = 0.03286803\n",
      "Iteration 122, loss = 0.03272742\n",
      "Iteration 123, loss = 0.03293854\n",
      "Iteration 124, loss = 0.03270009\n",
      "Iteration 125, loss = 0.03255189\n",
      "Iteration 126, loss = 0.03253130\n",
      "Iteration 127, loss = 0.03249516\n",
      "Iteration 128, loss = 0.03243130\n",
      "Iteration 129, loss = 0.03239382\n",
      "Iteration 130, loss = 0.03229660\n",
      "Iteration 131, loss = 0.03224379\n",
      "Iteration 132, loss = 0.03213517\n",
      "Iteration 133, loss = 0.03205962\n",
      "Iteration 134, loss = 0.03204445\n",
      "Iteration 135, loss = 0.03206616\n",
      "Iteration 136, loss = 0.03209272\n",
      "Iteration 137, loss = 0.03197746\n",
      "Iteration 138, loss = 0.03187121\n",
      "Iteration 139, loss = 0.03181289\n",
      "Iteration 140, loss = 0.03173102\n",
      "Iteration 141, loss = 0.03170228\n",
      "Iteration 142, loss = 0.03169862\n",
      "Iteration 143, loss = 0.03160026\n",
      "Iteration 144, loss = 0.03156727\n",
      "Iteration 145, loss = 0.03158036\n",
      "Iteration 146, loss = 0.03144599\n",
      "Iteration 147, loss = 0.03144404\n",
      "Iteration 148, loss = 0.03145704\n",
      "Iteration 149, loss = 0.03139568\n",
      "Iteration 150, loss = 0.03132757\n",
      "Iteration 151, loss = 0.03140472\n",
      "Iteration 152, loss = 0.03137588\n",
      "Iteration 153, loss = 0.03118334\n",
      "Iteration 154, loss = 0.03117232\n",
      "Iteration 155, loss = 0.03122809\n",
      "Iteration 156, loss = 0.03115628\n",
      "Iteration 157, loss = 0.03115768\n",
      "Iteration 158, loss = 0.03100948\n",
      "Iteration 159, loss = 0.03104651\n",
      "Iteration 160, loss = 0.03097360\n",
      "Iteration 161, loss = 0.03090293\n",
      "Iteration 162, loss = 0.03104091\n",
      "Iteration 163, loss = 0.03088335\n",
      "Iteration 164, loss = 0.03088879\n",
      "Iteration 165, loss = 0.03085127\n",
      "Iteration 166, loss = 0.03084811\n",
      "Iteration 167, loss = 0.03071197\n",
      "Iteration 168, loss = 0.03076050\n",
      "Iteration 169, loss = 0.03063991\n",
      "Iteration 170, loss = 0.03066223\n",
      "Iteration 171, loss = 0.03064700\n",
      "Iteration 172, loss = 0.03068194\n",
      "Iteration 173, loss = 0.03068613\n",
      "Iteration 174, loss = 0.03041581\n",
      "Iteration 175, loss = 0.03071940\n",
      "Iteration 176, loss = 0.03063666\n",
      "Iteration 177, loss = 0.03052719\n",
      "Iteration 178, loss = 0.03053191\n",
      "Iteration 179, loss = 0.03047461\n",
      "Iteration 180, loss = 0.03036329\n",
      "Iteration 181, loss = 0.03032839\n",
      "Iteration 182, loss = 0.03038968\n",
      "Iteration 183, loss = 0.03044159\n",
      "Iteration 184, loss = 0.03033376\n",
      "Iteration 185, loss = 0.03028584\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.99004658\n",
      "Iteration 2, loss = 0.93920207\n",
      "Iteration 3, loss = 0.89189878\n",
      "Iteration 4, loss = 0.84251718\n",
      "Iteration 5, loss = 0.79048249\n",
      "Iteration 6, loss = 0.73889859\n",
      "Iteration 7, loss = 0.68827449\n",
      "Iteration 8, loss = 0.64189504\n",
      "Iteration 9, loss = 0.59869418\n",
      "Iteration 10, loss = 0.55924896\n",
      "Iteration 11, loss = 0.52204093\n",
      "Iteration 12, loss = 0.48583106\n",
      "Iteration 13, loss = 0.45137163\n",
      "Iteration 14, loss = 0.41746916\n",
      "Iteration 15, loss = 0.38662651\n",
      "Iteration 16, loss = 0.35588191\n",
      "Iteration 17, loss = 0.32809542\n",
      "Iteration 18, loss = 0.30197668\n",
      "Iteration 19, loss = 0.27845679\n",
      "Iteration 20, loss = 0.25711152\n",
      "Iteration 21, loss = 0.23721274\n",
      "Iteration 22, loss = 0.21995967\n",
      "Iteration 23, loss = 0.20365375\n",
      "Iteration 24, loss = 0.18956711\n",
      "Iteration 25, loss = 0.17655410\n",
      "Iteration 26, loss = 0.16489830\n",
      "Iteration 27, loss = 0.15414365\n",
      "Iteration 28, loss = 0.14457042\n",
      "Iteration 29, loss = 0.13553662\n",
      "Iteration 30, loss = 0.12763287\n",
      "Iteration 31, loss = 0.12017327\n",
      "Iteration 32, loss = 0.11375325\n",
      "Iteration 33, loss = 0.10759462\n",
      "Iteration 34, loss = 0.10193513\n",
      "Iteration 35, loss = 0.09689128\n",
      "Iteration 36, loss = 0.09224675\n",
      "Iteration 37, loss = 0.08823230\n",
      "Iteration 38, loss = 0.08421411\n",
      "Iteration 39, loss = 0.08071895\n",
      "Iteration 40, loss = 0.07756217\n",
      "Iteration 41, loss = 0.07463083\n",
      "Iteration 42, loss = 0.07200501\n",
      "Iteration 43, loss = 0.06959413\n",
      "Iteration 44, loss = 0.06731164\n",
      "Iteration 45, loss = 0.06526988\n",
      "Iteration 46, loss = 0.06335244\n",
      "Iteration 47, loss = 0.06160169\n",
      "Iteration 48, loss = 0.06000893\n",
      "Iteration 49, loss = 0.05852893\n",
      "Iteration 50, loss = 0.05717002\n",
      "Iteration 51, loss = 0.05593104\n",
      "Iteration 52, loss = 0.05471580\n",
      "Iteration 53, loss = 0.05368989\n",
      "Iteration 54, loss = 0.05263852\n",
      "Iteration 55, loss = 0.05164131\n",
      "Iteration 56, loss = 0.05081624\n",
      "Iteration 57, loss = 0.04998782\n",
      "Iteration 58, loss = 0.04928270\n",
      "Iteration 59, loss = 0.04853271\n",
      "Iteration 60, loss = 0.04777029\n",
      "Iteration 61, loss = 0.04719512\n",
      "Iteration 62, loss = 0.04645387\n",
      "Iteration 63, loss = 0.04600763\n",
      "Iteration 64, loss = 0.04538900\n",
      "Iteration 65, loss = 0.04488762\n",
      "Iteration 66, loss = 0.04443183\n",
      "Iteration 67, loss = 0.04393928\n",
      "Iteration 68, loss = 0.04357766\n",
      "Iteration 69, loss = 0.04313272\n",
      "Iteration 70, loss = 0.04268590\n",
      "Iteration 71, loss = 0.04229037\n",
      "Iteration 72, loss = 0.04194187\n",
      "Iteration 73, loss = 0.04153106\n",
      "Iteration 74, loss = 0.04120153\n",
      "Iteration 75, loss = 0.04093469\n",
      "Iteration 76, loss = 0.04063389\n",
      "Iteration 77, loss = 0.04029305\n",
      "Iteration 78, loss = 0.04002397\n",
      "Iteration 79, loss = 0.03982835\n",
      "Iteration 80, loss = 0.03953450\n",
      "Iteration 81, loss = 0.03931681\n",
      "Iteration 82, loss = 0.03921132\n",
      "Iteration 83, loss = 0.03882537\n",
      "Iteration 84, loss = 0.03862703\n",
      "Iteration 85, loss = 0.03844538\n",
      "Iteration 86, loss = 0.03828534\n",
      "Iteration 87, loss = 0.03803807\n",
      "Iteration 88, loss = 0.03794794\n",
      "Iteration 89, loss = 0.03780413\n",
      "Iteration 90, loss = 0.03763023\n",
      "Iteration 91, loss = 0.03739998\n",
      "Iteration 92, loss = 0.03724276\n",
      "Iteration 93, loss = 0.03719151\n",
      "Iteration 94, loss = 0.03701335\n",
      "Iteration 95, loss = 0.03679761\n",
      "Iteration 96, loss = 0.03669346\n",
      "Iteration 97, loss = 0.03655018\n",
      "Iteration 98, loss = 0.03642107\n",
      "Iteration 99, loss = 0.03624177\n",
      "Iteration 100, loss = 0.03614118\n",
      "Iteration 101, loss = 0.03616259\n",
      "Iteration 102, loss = 0.03580699\n",
      "Iteration 103, loss = 0.03574334\n",
      "Iteration 104, loss = 0.03563376\n",
      "Iteration 105, loss = 0.03555763\n",
      "Iteration 106, loss = 0.03557267\n",
      "Iteration 107, loss = 0.03531651\n",
      "Iteration 108, loss = 0.03515199\n",
      "Iteration 109, loss = 0.03529751\n",
      "Iteration 110, loss = 0.03513629\n",
      "Iteration 111, loss = 0.03492901\n",
      "Iteration 112, loss = 0.03481966\n",
      "Iteration 113, loss = 0.03475454\n",
      "Iteration 114, loss = 0.03468303\n",
      "Iteration 115, loss = 0.03462361\n",
      "Iteration 116, loss = 0.03456064\n",
      "Iteration 117, loss = 0.03448854\n",
      "Iteration 118, loss = 0.03439082\n",
      "Iteration 119, loss = 0.03431327\n",
      "Iteration 120, loss = 0.03421636\n",
      "Iteration 121, loss = 0.03433943\n",
      "Iteration 122, loss = 0.03411391\n",
      "Iteration 123, loss = 0.03404548\n",
      "Iteration 124, loss = 0.03394490\n",
      "Iteration 125, loss = 0.03387098\n",
      "Iteration 126, loss = 0.03379319\n",
      "Iteration 127, loss = 0.03382462\n",
      "Iteration 128, loss = 0.03375974\n",
      "Iteration 129, loss = 0.03365257\n",
      "Iteration 130, loss = 0.03368486\n",
      "Iteration 131, loss = 0.03351506\n",
      "Iteration 132, loss = 0.03355128\n",
      "Iteration 133, loss = 0.03339772\n",
      "Iteration 134, loss = 0.03336796\n",
      "Iteration 135, loss = 0.03327848\n",
      "Iteration 136, loss = 0.03324205\n",
      "Iteration 137, loss = 0.03317778\n",
      "Iteration 138, loss = 0.03310990\n",
      "Iteration 139, loss = 0.03309427\n",
      "Iteration 140, loss = 0.03301424\n",
      "Iteration 141, loss = 0.03302212\n",
      "Iteration 142, loss = 0.03293831\n",
      "Iteration 143, loss = 0.03287080\n",
      "Iteration 144, loss = 0.03302455\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.97320247\n",
      "Iteration 2, loss = 0.91816382\n",
      "Iteration 3, loss = 0.86834733\n",
      "Iteration 4, loss = 0.81914469\n",
      "Iteration 5, loss = 0.77076876\n",
      "Iteration 6, loss = 0.72269408\n",
      "Iteration 7, loss = 0.67795111\n",
      "Iteration 8, loss = 0.63449104\n",
      "Iteration 9, loss = 0.59465644\n",
      "Iteration 10, loss = 0.55670389\n",
      "Iteration 11, loss = 0.51985131\n",
      "Iteration 12, loss = 0.48434832\n",
      "Iteration 13, loss = 0.44857212\n",
      "Iteration 14, loss = 0.41378114\n",
      "Iteration 15, loss = 0.38120741\n",
      "Iteration 16, loss = 0.34987703\n",
      "Iteration 17, loss = 0.32073358\n",
      "Iteration 18, loss = 0.29375266\n",
      "Iteration 19, loss = 0.26965597\n",
      "Iteration 20, loss = 0.24785121\n",
      "Iteration 21, loss = 0.22782105\n",
      "Iteration 22, loss = 0.21031930\n",
      "Iteration 23, loss = 0.19415402\n",
      "Iteration 24, loss = 0.17970411\n",
      "Iteration 25, loss = 0.16694136\n",
      "Iteration 26, loss = 0.15533000\n",
      "Iteration 27, loss = 0.14508483\n",
      "Iteration 28, loss = 0.13557187\n",
      "Iteration 29, loss = 0.12716979\n",
      "Iteration 30, loss = 0.11955916\n",
      "Iteration 31, loss = 0.11265247\n",
      "Iteration 32, loss = 0.10637690\n",
      "Iteration 33, loss = 0.10077934\n",
      "Iteration 34, loss = 0.09578233\n",
      "Iteration 35, loss = 0.09125594\n",
      "Iteration 36, loss = 0.08723396\n",
      "Iteration 37, loss = 0.08342546\n",
      "Iteration 38, loss = 0.08009001\n",
      "Iteration 39, loss = 0.07698265\n",
      "Iteration 40, loss = 0.07431560\n",
      "Iteration 41, loss = 0.07178189\n",
      "Iteration 42, loss = 0.06956864\n",
      "Iteration 43, loss = 0.06741714\n",
      "Iteration 44, loss = 0.06545426\n",
      "Iteration 45, loss = 0.06382073\n",
      "Iteration 46, loss = 0.06207614\n",
      "Iteration 47, loss = 0.06054604\n",
      "Iteration 48, loss = 0.05919636\n",
      "Iteration 49, loss = 0.05788497\n",
      "Iteration 50, loss = 0.05666261\n",
      "Iteration 51, loss = 0.05560052\n",
      "Iteration 52, loss = 0.05457576\n",
      "Iteration 53, loss = 0.05359160\n",
      "Iteration 54, loss = 0.05269577\n",
      "Iteration 55, loss = 0.05187821\n",
      "Iteration 56, loss = 0.05115477\n",
      "Iteration 57, loss = 0.05035064\n",
      "Iteration 58, loss = 0.04961071\n",
      "Iteration 59, loss = 0.04893944\n",
      "Iteration 60, loss = 0.04825161\n",
      "Iteration 61, loss = 0.04787037\n",
      "Iteration 62, loss = 0.04727580\n",
      "Iteration 63, loss = 0.04677161\n",
      "Iteration 64, loss = 0.04622500\n",
      "Iteration 65, loss = 0.04594812\n",
      "Iteration 66, loss = 0.04529754\n",
      "Iteration 67, loss = 0.04489769\n",
      "Iteration 68, loss = 0.04446866\n",
      "Iteration 69, loss = 0.04409491\n",
      "Iteration 70, loss = 0.04378236\n",
      "Iteration 71, loss = 0.04346408\n",
      "Iteration 72, loss = 0.04307131\n",
      "Iteration 73, loss = 0.04278738\n",
      "Iteration 74, loss = 0.04254883\n",
      "Iteration 75, loss = 0.04218250\n",
      "Iteration 76, loss = 0.04186031\n",
      "Iteration 77, loss = 0.04164212\n",
      "Iteration 78, loss = 0.04143520\n",
      "Iteration 79, loss = 0.04121977\n",
      "Iteration 80, loss = 0.04101102\n",
      "Iteration 81, loss = 0.04070359\n",
      "Iteration 82, loss = 0.04050253\n",
      "Iteration 83, loss = 0.04030808\n",
      "Iteration 84, loss = 0.04012910\n",
      "Iteration 85, loss = 0.03985154\n",
      "Iteration 86, loss = 0.03974965\n",
      "Iteration 87, loss = 0.03959851\n",
      "Iteration 88, loss = 0.03938132\n",
      "Iteration 89, loss = 0.03929926\n",
      "Iteration 90, loss = 0.03922752\n",
      "Iteration 91, loss = 0.03915716\n",
      "Iteration 92, loss = 0.03899799\n",
      "Iteration 93, loss = 0.03879648\n",
      "Iteration 94, loss = 0.03852770\n",
      "Iteration 95, loss = 0.03843275\n",
      "Iteration 96, loss = 0.03823020\n",
      "Iteration 97, loss = 0.03805852\n",
      "Iteration 98, loss = 0.03802637\n",
      "Iteration 99, loss = 0.03786677\n",
      "Iteration 100, loss = 0.03766381\n",
      "Iteration 101, loss = 0.03755244\n",
      "Iteration 102, loss = 0.03743982\n",
      "Iteration 103, loss = 0.03741078\n",
      "Iteration 104, loss = 0.03754231\n",
      "Iteration 105, loss = 0.03734718\n",
      "Iteration 106, loss = 0.03709163\n",
      "Iteration 107, loss = 0.03697125\n",
      "Iteration 108, loss = 0.03690584\n",
      "Iteration 109, loss = 0.03673853\n",
      "Iteration 110, loss = 0.03675654\n",
      "Iteration 111, loss = 0.03662279\n",
      "Iteration 112, loss = 0.03638317\n",
      "Iteration 113, loss = 0.03632414\n",
      "Iteration 114, loss = 0.03646719\n",
      "Iteration 115, loss = 0.03634482\n",
      "Iteration 116, loss = 0.03640359\n",
      "Iteration 117, loss = 0.03612735\n",
      "Iteration 118, loss = 0.03600135\n",
      "Iteration 119, loss = 0.03598242\n",
      "Iteration 120, loss = 0.03581283\n",
      "Iteration 121, loss = 0.03584129\n",
      "Iteration 122, loss = 0.03580723\n",
      "Iteration 123, loss = 0.03568587\n",
      "Iteration 124, loss = 0.03568398\n",
      "Iteration 125, loss = 0.03556989\n",
      "Iteration 126, loss = 0.03553966\n",
      "Iteration 127, loss = 0.03558491\n",
      "Iteration 128, loss = 0.03540018\n",
      "Iteration 129, loss = 0.03530627\n",
      "Iteration 130, loss = 0.03523677\n",
      "Iteration 131, loss = 0.03515857\n",
      "Iteration 132, loss = 0.03510851\n",
      "Iteration 133, loss = 0.03514086\n",
      "Iteration 134, loss = 0.03504059\n",
      "Iteration 135, loss = 0.03489717\n",
      "Iteration 136, loss = 0.03512575\n",
      "Iteration 137, loss = 0.03497339\n",
      "Iteration 138, loss = 0.03485122\n",
      "Iteration 139, loss = 0.03479885\n",
      "Iteration 140, loss = 0.03470284\n",
      "Iteration 141, loss = 0.03463116\n",
      "Iteration 142, loss = 0.03491300\n",
      "Iteration 143, loss = 0.03474681\n",
      "Iteration 144, loss = 0.03465390\n",
      "Iteration 145, loss = 0.03455760\n",
      "Iteration 146, loss = 0.03445866\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.99686000\n",
      "Iteration 2, loss = 0.94221136\n",
      "Iteration 3, loss = 0.88919943\n",
      "Iteration 4, loss = 0.83577823\n",
      "Iteration 5, loss = 0.77982555\n",
      "Iteration 6, loss = 0.72541316\n",
      "Iteration 7, loss = 0.67350977\n",
      "Iteration 8, loss = 0.62637131\n",
      "Iteration 9, loss = 0.58289129\n",
      "Iteration 10, loss = 0.54374205\n",
      "Iteration 11, loss = 0.50674959\n",
      "Iteration 12, loss = 0.47196657\n",
      "Iteration 13, loss = 0.43815261\n",
      "Iteration 14, loss = 0.40630432\n",
      "Iteration 15, loss = 0.37584020\n",
      "Iteration 16, loss = 0.34678016\n",
      "Iteration 17, loss = 0.31966115\n",
      "Iteration 18, loss = 0.29486937\n",
      "Iteration 19, loss = 0.27172500\n",
      "Iteration 20, loss = 0.25035010\n",
      "Iteration 21, loss = 0.23110822\n",
      "Iteration 22, loss = 0.21331716\n",
      "Iteration 23, loss = 0.19759107\n",
      "Iteration 24, loss = 0.18326119\n",
      "Iteration 25, loss = 0.17000093\n",
      "Iteration 26, loss = 0.15842840\n",
      "Iteration 27, loss = 0.14787659\n",
      "Iteration 28, loss = 0.13824364\n",
      "Iteration 29, loss = 0.12985156\n",
      "Iteration 30, loss = 0.12209836\n",
      "Iteration 31, loss = 0.11505129\n",
      "Iteration 32, loss = 0.10882650\n",
      "Iteration 33, loss = 0.10296246\n",
      "Iteration 34, loss = 0.09801213\n",
      "Iteration 35, loss = 0.09339908\n",
      "Iteration 36, loss = 0.08915884\n",
      "Iteration 37, loss = 0.08547891\n",
      "Iteration 38, loss = 0.08193588\n",
      "Iteration 39, loss = 0.07881160\n",
      "Iteration 40, loss = 0.07602227\n",
      "Iteration 41, loss = 0.07350797\n",
      "Iteration 42, loss = 0.07109894\n",
      "Iteration 43, loss = 0.06891254\n",
      "Iteration 44, loss = 0.06699232\n",
      "Iteration 45, loss = 0.06507738\n",
      "Iteration 46, loss = 0.06348849\n",
      "Iteration 47, loss = 0.06192196\n",
      "Iteration 48, loss = 0.06036063\n",
      "Iteration 49, loss = 0.05907507\n",
      "Iteration 50, loss = 0.05783434\n",
      "Iteration 51, loss = 0.05659776\n",
      "Iteration 52, loss = 0.05550539\n",
      "Iteration 53, loss = 0.05465298\n",
      "Iteration 54, loss = 0.05373258\n",
      "Iteration 55, loss = 0.05281418\n",
      "Iteration 56, loss = 0.05186924\n",
      "Iteration 57, loss = 0.05109402\n",
      "Iteration 58, loss = 0.05042314\n",
      "Iteration 59, loss = 0.04962524\n",
      "Iteration 60, loss = 0.04898223\n",
      "Iteration 61, loss = 0.04838693\n",
      "Iteration 62, loss = 0.04780691\n",
      "Iteration 63, loss = 0.04732555\n",
      "Iteration 64, loss = 0.04691831\n",
      "Iteration 65, loss = 0.04640643\n",
      "Iteration 66, loss = 0.04607677\n",
      "Iteration 67, loss = 0.04554236\n",
      "Iteration 68, loss = 0.04511221\n",
      "Iteration 69, loss = 0.04465769\n",
      "Iteration 70, loss = 0.04420433\n",
      "Iteration 71, loss = 0.04384129\n",
      "Iteration 72, loss = 0.04350827\n",
      "Iteration 73, loss = 0.04316508\n",
      "Iteration 74, loss = 0.04287622\n",
      "Iteration 75, loss = 0.04272896\n",
      "Iteration 76, loss = 0.04235709\n",
      "Iteration 77, loss = 0.04214336\n",
      "Iteration 78, loss = 0.04175775\n",
      "Iteration 79, loss = 0.04157763\n",
      "Iteration 80, loss = 0.04153335\n",
      "Iteration 81, loss = 0.04107851\n",
      "Iteration 82, loss = 0.04083584\n",
      "Iteration 83, loss = 0.04060683\n",
      "Iteration 84, loss = 0.04054108\n",
      "Iteration 85, loss = 0.04018751\n",
      "Iteration 86, loss = 0.03998615\n",
      "Iteration 87, loss = 0.03981241\n",
      "Iteration 88, loss = 0.03963629\n",
      "Iteration 89, loss = 0.03951432\n",
      "Iteration 90, loss = 0.03943066\n",
      "Iteration 91, loss = 0.03921700\n",
      "Iteration 92, loss = 0.03905069\n",
      "Iteration 93, loss = 0.03888590\n",
      "Iteration 94, loss = 0.03873272\n",
      "Iteration 95, loss = 0.03864837\n",
      "Iteration 96, loss = 0.03859028\n",
      "Iteration 97, loss = 0.03838204\n",
      "Iteration 98, loss = 0.03819777\n",
      "Iteration 99, loss = 0.03813120\n",
      "Iteration 100, loss = 0.03804033\n",
      "Iteration 101, loss = 0.03789647\n",
      "Iteration 102, loss = 0.03779687\n",
      "Iteration 103, loss = 0.03761278\n",
      "Iteration 104, loss = 0.03748459\n",
      "Iteration 105, loss = 0.03737407\n",
      "Iteration 106, loss = 0.03728223\n",
      "Iteration 107, loss = 0.03723832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 108, loss = 0.03716463\n",
      "Iteration 109, loss = 0.03694916\n",
      "Iteration 110, loss = 0.03685099\n",
      "Iteration 111, loss = 0.03690758\n",
      "Iteration 112, loss = 0.03680892\n",
      "Iteration 113, loss = 0.03670729\n",
      "Iteration 114, loss = 0.03667772\n",
      "Iteration 115, loss = 0.03655031\n",
      "Iteration 116, loss = 0.03640601\n",
      "Iteration 117, loss = 0.03638674\n",
      "Iteration 118, loss = 0.03620444\n",
      "Iteration 119, loss = 0.03612276\n",
      "Iteration 120, loss = 0.03619012\n",
      "Iteration 121, loss = 0.03612695\n",
      "Iteration 122, loss = 0.03598544\n",
      "Iteration 123, loss = 0.03587752\n",
      "Iteration 124, loss = 0.03579685\n",
      "Iteration 125, loss = 0.03581280\n",
      "Iteration 126, loss = 0.03566784\n",
      "Iteration 127, loss = 0.03559813\n",
      "Iteration 128, loss = 0.03552481\n",
      "Iteration 129, loss = 0.03544345\n",
      "Iteration 130, loss = 0.03541309\n",
      "Iteration 131, loss = 0.03539162\n",
      "Iteration 132, loss = 0.03540143\n",
      "Iteration 133, loss = 0.03533878\n",
      "Iteration 134, loss = 0.03523785\n",
      "Iteration 135, loss = 0.03521538\n",
      "Iteration 136, loss = 0.03526765\n",
      "Iteration 137, loss = 0.03519108\n",
      "Iteration 138, loss = 0.03513979\n",
      "Iteration 139, loss = 0.03502833\n",
      "Iteration 140, loss = 0.03502597\n",
      "Iteration 141, loss = 0.03487780\n",
      "Iteration 142, loss = 0.03489319\n",
      "Iteration 143, loss = 0.03485128\n",
      "Iteration 144, loss = 0.03484507\n",
      "Iteration 145, loss = 0.03465390\n",
      "Iteration 146, loss = 0.03460966\n",
      "Iteration 147, loss = 0.03471371\n",
      "Iteration 148, loss = 0.03460074\n",
      "Iteration 149, loss = 0.03459156\n",
      "Iteration 150, loss = 0.03463486\n",
      "Iteration 151, loss = 0.03450663\n",
      "Iteration 152, loss = 0.03441256\n",
      "Iteration 153, loss = 0.03433304\n",
      "Iteration 154, loss = 0.03427055\n",
      "Iteration 155, loss = 0.03425625\n",
      "Iteration 156, loss = 0.03428034\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.19807521\n",
      "Iteration 2, loss = 1.13430801\n",
      "Iteration 3, loss = 1.07358486\n",
      "Iteration 4, loss = 1.01136404\n",
      "Iteration 5, loss = 0.94496878\n",
      "Iteration 6, loss = 0.87663438\n",
      "Iteration 7, loss = 0.80915539\n",
      "Iteration 8, loss = 0.74624141\n",
      "Iteration 9, loss = 0.68765492\n",
      "Iteration 10, loss = 0.63633607\n",
      "Iteration 11, loss = 0.59071799\n",
      "Iteration 12, loss = 0.54945132\n",
      "Iteration 13, loss = 0.51145767\n",
      "Iteration 14, loss = 0.47542738\n",
      "Iteration 15, loss = 0.44070776\n",
      "Iteration 16, loss = 0.40745984\n",
      "Iteration 17, loss = 0.37695486\n",
      "Iteration 18, loss = 0.34802707\n",
      "Iteration 19, loss = 0.32130537\n",
      "Iteration 20, loss = 0.29631661\n",
      "Iteration 21, loss = 0.27360193\n",
      "Iteration 22, loss = 0.25241718\n",
      "Iteration 23, loss = 0.23382906\n",
      "Iteration 24, loss = 0.21648746\n",
      "Iteration 25, loss = 0.20083345\n",
      "Iteration 26, loss = 0.18649832\n",
      "Iteration 27, loss = 0.17387872\n",
      "Iteration 28, loss = 0.16254126\n",
      "Iteration 29, loss = 0.15211347\n",
      "Iteration 30, loss = 0.14267335\n",
      "Iteration 31, loss = 0.13429104\n",
      "Iteration 32, loss = 0.12663013\n",
      "Iteration 33, loss = 0.11971520\n",
      "Iteration 34, loss = 0.11324577\n",
      "Iteration 35, loss = 0.10753560\n",
      "Iteration 36, loss = 0.10235596\n",
      "Iteration 37, loss = 0.09775251\n",
      "Iteration 38, loss = 0.09340228\n",
      "Iteration 39, loss = 0.08955613\n",
      "Iteration 40, loss = 0.08601226\n",
      "Iteration 41, loss = 0.08290533\n",
      "Iteration 42, loss = 0.07991322\n",
      "Iteration 43, loss = 0.07718261\n",
      "Iteration 44, loss = 0.07471501\n",
      "Iteration 45, loss = 0.07251815\n",
      "Iteration 46, loss = 0.07043500\n",
      "Iteration 47, loss = 0.06844450\n",
      "Iteration 48, loss = 0.06663341\n",
      "Iteration 49, loss = 0.06511628\n",
      "Iteration 50, loss = 0.06354708\n",
      "Iteration 51, loss = 0.06212720\n",
      "Iteration 52, loss = 0.06077658\n",
      "Iteration 53, loss = 0.05954482\n",
      "Iteration 54, loss = 0.05843723\n",
      "Iteration 55, loss = 0.05739904\n",
      "Iteration 56, loss = 0.05642859\n",
      "Iteration 57, loss = 0.05537725\n",
      "Iteration 58, loss = 0.05456539\n",
      "Iteration 59, loss = 0.05378861\n",
      "Iteration 60, loss = 0.05297445\n",
      "Iteration 61, loss = 0.05224321\n",
      "Iteration 62, loss = 0.05172623\n",
      "Iteration 63, loss = 0.05108493\n",
      "Iteration 64, loss = 0.05040610\n",
      "Iteration 65, loss = 0.04976183\n",
      "Iteration 66, loss = 0.04932590\n",
      "Iteration 67, loss = 0.04871262\n",
      "Iteration 68, loss = 0.04820551\n",
      "Iteration 69, loss = 0.04774547\n",
      "Iteration 70, loss = 0.04732703\n",
      "Iteration 71, loss = 0.04693727\n",
      "Iteration 72, loss = 0.04659622\n",
      "Iteration 73, loss = 0.04611090\n",
      "Iteration 74, loss = 0.04578445\n",
      "Iteration 75, loss = 0.04538138\n",
      "Iteration 76, loss = 0.04499998\n",
      "Iteration 77, loss = 0.04481268\n",
      "Iteration 78, loss = 0.04437408\n",
      "Iteration 79, loss = 0.04409916\n",
      "Iteration 80, loss = 0.04382643\n",
      "Iteration 81, loss = 0.04358931\n",
      "Iteration 82, loss = 0.04343169\n",
      "Iteration 83, loss = 0.04314737\n",
      "Iteration 84, loss = 0.04281829\n",
      "Iteration 85, loss = 0.04258641\n",
      "Iteration 86, loss = 0.04238744\n",
      "Iteration 87, loss = 0.04218048\n",
      "Iteration 88, loss = 0.04196580\n",
      "Iteration 89, loss = 0.04171930\n",
      "Iteration 90, loss = 0.04157240\n",
      "Iteration 91, loss = 0.04143638\n",
      "Iteration 92, loss = 0.04120367\n",
      "Iteration 93, loss = 0.04110007\n",
      "Iteration 94, loss = 0.04083885\n",
      "Iteration 95, loss = 0.04090474\n",
      "Iteration 96, loss = 0.04089325\n",
      "Iteration 97, loss = 0.04070607\n",
      "Iteration 98, loss = 0.04044117\n",
      "Iteration 99, loss = 0.04024056\n",
      "Iteration 100, loss = 0.04006855\n",
      "Iteration 101, loss = 0.03991693\n",
      "Iteration 102, loss = 0.03980318\n",
      "Iteration 103, loss = 0.03968725\n",
      "Iteration 104, loss = 0.03956472\n",
      "Iteration 105, loss = 0.03949354\n",
      "Iteration 106, loss = 0.03929600\n",
      "Iteration 107, loss = 0.03927176\n",
      "Iteration 108, loss = 0.03906287\n",
      "Iteration 109, loss = 0.03897609\n",
      "Iteration 110, loss = 0.03893368\n",
      "Iteration 111, loss = 0.03890782\n",
      "Iteration 112, loss = 0.03874282\n",
      "Iteration 113, loss = 0.03860465\n",
      "Iteration 114, loss = 0.03857344\n",
      "Iteration 115, loss = 0.03845849\n",
      "Iteration 116, loss = 0.03834918\n",
      "Iteration 117, loss = 0.03825489\n",
      "Iteration 118, loss = 0.03826407\n",
      "Iteration 119, loss = 0.03811324\n",
      "Iteration 120, loss = 0.03803796\n",
      "Iteration 121, loss = 0.03796751\n",
      "Iteration 122, loss = 0.03798434\n",
      "Iteration 123, loss = 0.03782573\n",
      "Iteration 124, loss = 0.03770095\n",
      "Iteration 125, loss = 0.03765755\n",
      "Iteration 126, loss = 0.03769794\n",
      "Iteration 127, loss = 0.03758818\n",
      "Iteration 128, loss = 0.03755545\n",
      "Iteration 129, loss = 0.03739335\n",
      "Iteration 130, loss = 0.03738497\n",
      "Iteration 131, loss = 0.03728296\n",
      "Iteration 132, loss = 0.03751590\n",
      "Iteration 133, loss = 0.03736038\n",
      "Iteration 134, loss = 0.03722317\n",
      "Iteration 135, loss = 0.03722622\n",
      "Iteration 136, loss = 0.03698125\n",
      "Iteration 137, loss = 0.03706960\n",
      "Iteration 138, loss = 0.03713069\n",
      "Iteration 139, loss = 0.03698231\n",
      "Iteration 140, loss = 0.03702782\n",
      "Iteration 141, loss = 0.03680432\n",
      "Iteration 142, loss = 0.03681007\n",
      "Iteration 143, loss = 0.03666067\n",
      "Iteration 144, loss = 0.03662976\n",
      "Iteration 145, loss = 0.03669306\n",
      "Iteration 146, loss = 0.03651536\n",
      "Iteration 147, loss = 0.03649281\n",
      "Iteration 148, loss = 0.03646169\n",
      "Iteration 149, loss = 0.03646574\n",
      "Iteration 150, loss = 0.03644822\n",
      "Iteration 151, loss = 0.03644311\n",
      "Iteration 152, loss = 0.03620619\n",
      "Iteration 153, loss = 0.03621055\n",
      "Iteration 154, loss = 0.03630529\n",
      "Iteration 155, loss = 0.03625647\n",
      "Iteration 156, loss = 0.03631306\n",
      "Iteration 157, loss = 0.03608747\n",
      "Iteration 158, loss = 0.03617596\n",
      "Iteration 159, loss = 0.03602664\n",
      "Iteration 160, loss = 0.03609006\n",
      "Iteration 161, loss = 0.03616864\n",
      "Iteration 162, loss = 0.03594371\n",
      "Iteration 163, loss = 0.03618442\n",
      "Iteration 164, loss = 0.03588976\n",
      "Iteration 165, loss = 0.03580338\n",
      "Iteration 166, loss = 0.03581400\n",
      "Iteration 167, loss = 0.03585864\n",
      "Iteration 168, loss = 0.03602514\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.11114765\n",
      "Iteration 2, loss = 1.04462911\n",
      "Iteration 3, loss = 0.98129431\n",
      "Iteration 4, loss = 0.91578992\n",
      "Iteration 5, loss = 0.84571063\n",
      "Iteration 6, loss = 0.77666265\n",
      "Iteration 7, loss = 0.71179929\n",
      "Iteration 8, loss = 0.65461929\n",
      "Iteration 9, loss = 0.60433840\n",
      "Iteration 10, loss = 0.56161290\n",
      "Iteration 11, loss = 0.52371399\n",
      "Iteration 12, loss = 0.48887880\n",
      "Iteration 13, loss = 0.45617777\n",
      "Iteration 14, loss = 0.42426284\n",
      "Iteration 15, loss = 0.39426224\n",
      "Iteration 16, loss = 0.36573565\n",
      "Iteration 17, loss = 0.33834490\n",
      "Iteration 18, loss = 0.31282883\n",
      "Iteration 19, loss = 0.28903647\n",
      "Iteration 20, loss = 0.26703386\n",
      "Iteration 21, loss = 0.24691013\n",
      "Iteration 22, loss = 0.22827279\n",
      "Iteration 23, loss = 0.21162114\n",
      "Iteration 24, loss = 0.19632283\n",
      "Iteration 25, loss = 0.18265640\n",
      "Iteration 26, loss = 0.17047081\n",
      "Iteration 27, loss = 0.15916356\n",
      "Iteration 28, loss = 0.14886029\n",
      "Iteration 29, loss = 0.13989852\n",
      "Iteration 30, loss = 0.13169046\n",
      "Iteration 31, loss = 0.12375259\n",
      "Iteration 32, loss = 0.11713056\n",
      "Iteration 33, loss = 0.11051237\n",
      "Iteration 34, loss = 0.10500253\n",
      "Iteration 35, loss = 0.09961554\n",
      "Iteration 36, loss = 0.09489326\n",
      "Iteration 37, loss = 0.09047710\n",
      "Iteration 38, loss = 0.08640043\n",
      "Iteration 39, loss = 0.08288041\n",
      "Iteration 40, loss = 0.07947718\n",
      "Iteration 41, loss = 0.07646232\n",
      "Iteration 42, loss = 0.07359283\n",
      "Iteration 43, loss = 0.07107143\n",
      "Iteration 44, loss = 0.06869976\n",
      "Iteration 45, loss = 0.06653337\n",
      "Iteration 46, loss = 0.06454502\n",
      "Iteration 47, loss = 0.06266816\n",
      "Iteration 48, loss = 0.06100034\n",
      "Iteration 49, loss = 0.05955849\n",
      "Iteration 50, loss = 0.05808198\n",
      "Iteration 51, loss = 0.05671961\n",
      "Iteration 52, loss = 0.05544703\n",
      "Iteration 53, loss = 0.05426365\n",
      "Iteration 54, loss = 0.05312725\n",
      "Iteration 55, loss = 0.05214501\n",
      "Iteration 56, loss = 0.05121843\n",
      "Iteration 57, loss = 0.05035271\n",
      "Iteration 58, loss = 0.04959871\n",
      "Iteration 59, loss = 0.04870852\n",
      "Iteration 60, loss = 0.04796135\n",
      "Iteration 61, loss = 0.04739115\n",
      "Iteration 62, loss = 0.04666470\n",
      "Iteration 63, loss = 0.04617401\n",
      "Iteration 64, loss = 0.04555409\n",
      "Iteration 65, loss = 0.04498324\n",
      "Iteration 66, loss = 0.04456047\n",
      "Iteration 67, loss = 0.04394708\n",
      "Iteration 68, loss = 0.04358100\n",
      "Iteration 69, loss = 0.04305844\n",
      "Iteration 70, loss = 0.04269153\n",
      "Iteration 71, loss = 0.04226428\n",
      "Iteration 72, loss = 0.04198691\n",
      "Iteration 73, loss = 0.04168656\n",
      "Iteration 74, loss = 0.04129950\n",
      "Iteration 75, loss = 0.04095067\n",
      "Iteration 76, loss = 0.04067132\n",
      "Iteration 77, loss = 0.04038272\n",
      "Iteration 78, loss = 0.04013403\n",
      "Iteration 79, loss = 0.03975512\n",
      "Iteration 80, loss = 0.03946945\n",
      "Iteration 81, loss = 0.03925144\n",
      "Iteration 82, loss = 0.03898023\n",
      "Iteration 83, loss = 0.03873852\n",
      "Iteration 84, loss = 0.03851315\n",
      "Iteration 85, loss = 0.03835476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 86, loss = 0.03813642\n",
      "Iteration 87, loss = 0.03791576\n",
      "Iteration 88, loss = 0.03771681\n",
      "Iteration 89, loss = 0.03773986\n",
      "Iteration 90, loss = 0.03748815\n",
      "Iteration 91, loss = 0.03724440\n",
      "Iteration 92, loss = 0.03714230\n",
      "Iteration 93, loss = 0.03685957\n",
      "Iteration 94, loss = 0.03695043\n",
      "Iteration 95, loss = 0.03663678\n",
      "Iteration 96, loss = 0.03644703\n",
      "Iteration 97, loss = 0.03626164\n",
      "Iteration 98, loss = 0.03640300\n",
      "Iteration 99, loss = 0.03608018\n",
      "Iteration 100, loss = 0.03590387\n",
      "Iteration 101, loss = 0.03582984\n",
      "Iteration 102, loss = 0.03578050\n",
      "Iteration 103, loss = 0.03573088\n",
      "Iteration 104, loss = 0.03551760\n",
      "Iteration 105, loss = 0.03542407\n",
      "Iteration 106, loss = 0.03526123\n",
      "Iteration 107, loss = 0.03518097\n",
      "Iteration 108, loss = 0.03518413\n",
      "Iteration 109, loss = 0.03506728\n",
      "Iteration 110, loss = 0.03484861\n",
      "Iteration 111, loss = 0.03480150\n",
      "Iteration 112, loss = 0.03465807\n",
      "Iteration 113, loss = 0.03461604\n",
      "Iteration 114, loss = 0.03468020\n",
      "Iteration 115, loss = 0.03452396\n",
      "Iteration 116, loss = 0.03446408\n",
      "Iteration 117, loss = 0.03433893\n",
      "Iteration 118, loss = 0.03429066\n",
      "Iteration 119, loss = 0.03413411\n",
      "Iteration 120, loss = 0.03416378\n",
      "Iteration 121, loss = 0.03398986\n",
      "Iteration 122, loss = 0.03392099\n",
      "Iteration 123, loss = 0.03393078\n",
      "Iteration 124, loss = 0.03395481\n",
      "Iteration 125, loss = 0.03380337\n",
      "Iteration 126, loss = 0.03369122\n",
      "Iteration 127, loss = 0.03359624\n",
      "Iteration 128, loss = 0.03351230\n",
      "Iteration 129, loss = 0.03356391\n",
      "Iteration 130, loss = 0.03339263\n",
      "Iteration 131, loss = 0.03345190\n",
      "Iteration 132, loss = 0.03334133\n",
      "Iteration 133, loss = 0.03331552\n",
      "Iteration 134, loss = 0.03322473\n",
      "Iteration 135, loss = 0.03317344\n",
      "Iteration 136, loss = 0.03315925\n",
      "Iteration 137, loss = 0.03309415\n",
      "Iteration 138, loss = 0.03302446\n",
      "Iteration 139, loss = 0.03298373\n",
      "Iteration 140, loss = 0.03284789\n",
      "Iteration 141, loss = 0.03284034\n",
      "Iteration 142, loss = 0.03282871\n",
      "Iteration 143, loss = 0.03277296\n",
      "Iteration 144, loss = 0.03272903\n",
      "Iteration 145, loss = 0.03275630\n",
      "Iteration 146, loss = 0.03272754\n",
      "Iteration 147, loss = 0.03266326\n",
      "Iteration 148, loss = 0.03255050\n",
      "Iteration 149, loss = 0.03248830\n",
      "Iteration 150, loss = 0.03235305\n",
      "Iteration 151, loss = 0.03253961\n",
      "Iteration 152, loss = 0.03256633\n",
      "Iteration 153, loss = 0.03236910\n",
      "Iteration 154, loss = 0.03231951\n",
      "Iteration 155, loss = 0.03224849\n",
      "Iteration 156, loss = 0.03226530\n",
      "Iteration 157, loss = 0.03229223\n",
      "Iteration 158, loss = 0.03218737\n",
      "Iteration 159, loss = 0.03212635\n",
      "Iteration 160, loss = 0.03214026\n",
      "Iteration 161, loss = 0.03208756\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.21096737\n",
      "Iteration 2, loss = 1.13540443\n",
      "Iteration 3, loss = 1.06825677\n",
      "Iteration 4, loss = 1.00097781\n",
      "Iteration 5, loss = 0.93198871\n",
      "Iteration 6, loss = 0.86144444\n",
      "Iteration 7, loss = 0.79342410\n",
      "Iteration 8, loss = 0.72764404\n",
      "Iteration 9, loss = 0.66886554\n",
      "Iteration 10, loss = 0.61643730\n",
      "Iteration 11, loss = 0.56971992\n",
      "Iteration 12, loss = 0.52722741\n",
      "Iteration 13, loss = 0.48819727\n",
      "Iteration 14, loss = 0.45124291\n",
      "Iteration 15, loss = 0.41740820\n",
      "Iteration 16, loss = 0.38464783\n",
      "Iteration 17, loss = 0.35456739\n",
      "Iteration 18, loss = 0.32673572\n",
      "Iteration 19, loss = 0.30139212\n",
      "Iteration 20, loss = 0.27843018\n",
      "Iteration 21, loss = 0.25736282\n",
      "Iteration 22, loss = 0.23800038\n",
      "Iteration 23, loss = 0.22092460\n",
      "Iteration 24, loss = 0.20546247\n",
      "Iteration 25, loss = 0.19123601\n",
      "Iteration 26, loss = 0.17855112\n",
      "Iteration 27, loss = 0.16719510\n",
      "Iteration 28, loss = 0.15675027\n",
      "Iteration 29, loss = 0.14724449\n",
      "Iteration 30, loss = 0.13864844\n",
      "Iteration 31, loss = 0.13089793\n",
      "Iteration 32, loss = 0.12363725\n",
      "Iteration 33, loss = 0.11699725\n",
      "Iteration 34, loss = 0.11118406\n",
      "Iteration 35, loss = 0.10588808\n",
      "Iteration 36, loss = 0.10084531\n",
      "Iteration 37, loss = 0.09613437\n",
      "Iteration 38, loss = 0.09221048\n",
      "Iteration 39, loss = 0.08842827\n",
      "Iteration 40, loss = 0.08497201\n",
      "Iteration 41, loss = 0.08179968\n",
      "Iteration 42, loss = 0.07894352\n",
      "Iteration 43, loss = 0.07636655\n",
      "Iteration 44, loss = 0.07385762\n",
      "Iteration 45, loss = 0.07166636\n",
      "Iteration 46, loss = 0.06956810\n",
      "Iteration 47, loss = 0.06764502\n",
      "Iteration 48, loss = 0.06588366\n",
      "Iteration 49, loss = 0.06421568\n",
      "Iteration 50, loss = 0.06269091\n",
      "Iteration 51, loss = 0.06143608\n",
      "Iteration 52, loss = 0.06005409\n",
      "Iteration 53, loss = 0.05885455\n",
      "Iteration 54, loss = 0.05772933\n",
      "Iteration 55, loss = 0.05670877\n",
      "Iteration 56, loss = 0.05577230\n",
      "Iteration 57, loss = 0.05482582\n",
      "Iteration 58, loss = 0.05405119\n",
      "Iteration 59, loss = 0.05321302\n",
      "Iteration 60, loss = 0.05258945\n",
      "Iteration 61, loss = 0.05176897\n",
      "Iteration 62, loss = 0.05111797\n",
      "Iteration 63, loss = 0.05042495\n",
      "Iteration 64, loss = 0.04985563\n",
      "Iteration 65, loss = 0.04937793\n",
      "Iteration 66, loss = 0.04887549\n",
      "Iteration 67, loss = 0.04848673\n",
      "Iteration 68, loss = 0.04789031\n",
      "Iteration 69, loss = 0.04746981\n",
      "Iteration 70, loss = 0.04708684\n",
      "Iteration 71, loss = 0.04659861\n",
      "Iteration 72, loss = 0.04621034\n",
      "Iteration 73, loss = 0.04592303\n",
      "Iteration 74, loss = 0.04558028\n",
      "Iteration 75, loss = 0.04512532\n",
      "Iteration 76, loss = 0.04489221\n",
      "Iteration 77, loss = 0.04461223\n",
      "Iteration 78, loss = 0.04419922\n",
      "Iteration 79, loss = 0.04394396\n",
      "Iteration 80, loss = 0.04375670\n",
      "Iteration 81, loss = 0.04353589\n",
      "Iteration 82, loss = 0.04323739\n",
      "Iteration 83, loss = 0.04297073\n",
      "Iteration 84, loss = 0.04276138\n",
      "Iteration 85, loss = 0.04260021\n",
      "Iteration 86, loss = 0.04234948\n",
      "Iteration 87, loss = 0.04214313\n",
      "Iteration 88, loss = 0.04194578\n",
      "Iteration 89, loss = 0.04177052\n",
      "Iteration 90, loss = 0.04153009\n",
      "Iteration 91, loss = 0.04137619\n",
      "Iteration 92, loss = 0.04124117\n",
      "Iteration 93, loss = 0.04116996\n",
      "Iteration 94, loss = 0.04103078\n",
      "Iteration 95, loss = 0.04087140\n",
      "Iteration 96, loss = 0.04059884\n",
      "Iteration 97, loss = 0.04052031\n",
      "Iteration 98, loss = 0.04041200\n",
      "Iteration 99, loss = 0.04031637\n",
      "Iteration 100, loss = 0.04025660\n",
      "Iteration 101, loss = 0.04007099\n",
      "Iteration 102, loss = 0.04004281\n",
      "Iteration 103, loss = 0.03973616\n",
      "Iteration 104, loss = 0.03963988\n",
      "Iteration 105, loss = 0.03960799\n",
      "Iteration 106, loss = 0.03950115\n",
      "Iteration 107, loss = 0.03929809\n",
      "Iteration 108, loss = 0.03919925\n",
      "Iteration 109, loss = 0.03905635\n",
      "Iteration 110, loss = 0.03895397\n",
      "Iteration 111, loss = 0.03884131\n",
      "Iteration 112, loss = 0.03878360\n",
      "Iteration 113, loss = 0.03871416\n",
      "Iteration 114, loss = 0.03858868\n",
      "Iteration 115, loss = 0.03857006\n",
      "Iteration 116, loss = 0.03842437\n",
      "Iteration 117, loss = 0.03838526\n",
      "Iteration 118, loss = 0.03829789\n",
      "Iteration 119, loss = 0.03825036\n",
      "Iteration 120, loss = 0.03813768\n",
      "Iteration 121, loss = 0.03809975\n",
      "Iteration 122, loss = 0.03815653\n",
      "Iteration 123, loss = 0.03796446\n",
      "Iteration 124, loss = 0.03789315\n",
      "Iteration 125, loss = 0.03775829\n",
      "Iteration 126, loss = 0.03767804\n",
      "Iteration 127, loss = 0.03775677\n",
      "Iteration 128, loss = 0.03763100\n",
      "Iteration 129, loss = 0.03750650\n",
      "Iteration 130, loss = 0.03750584\n",
      "Iteration 131, loss = 0.03743942\n",
      "Iteration 132, loss = 0.03733756\n",
      "Iteration 133, loss = 0.03728698\n",
      "Iteration 134, loss = 0.03727975\n",
      "Iteration 135, loss = 0.03724730\n",
      "Iteration 136, loss = 0.03731072\n",
      "Iteration 137, loss = 0.03717833\n",
      "Iteration 138, loss = 0.03704054\n",
      "Iteration 139, loss = 0.03700336\n",
      "Iteration 140, loss = 0.03689382\n",
      "Iteration 141, loss = 0.03688373\n",
      "Iteration 142, loss = 0.03682069\n",
      "Iteration 143, loss = 0.03686049\n",
      "Iteration 144, loss = 0.03673113\n",
      "Iteration 145, loss = 0.03669489\n",
      "Iteration 146, loss = 0.03670364\n",
      "Iteration 147, loss = 0.03660020\n",
      "Iteration 148, loss = 0.03655649\n",
      "Iteration 149, loss = 0.03653497\n",
      "Iteration 150, loss = 0.03665506\n",
      "Iteration 151, loss = 0.03648787\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.11815554\n",
      "Iteration 2, loss = 1.06671792\n",
      "Iteration 3, loss = 1.01758524\n",
      "Iteration 4, loss = 0.96194472\n",
      "Iteration 5, loss = 0.90199487\n",
      "Iteration 6, loss = 0.83667402\n",
      "Iteration 7, loss = 0.77207214\n",
      "Iteration 8, loss = 0.71056946\n",
      "Iteration 9, loss = 0.65389229\n",
      "Iteration 10, loss = 0.60407143\n",
      "Iteration 11, loss = 0.56066665\n",
      "Iteration 12, loss = 0.52174143\n",
      "Iteration 13, loss = 0.48533129\n",
      "Iteration 14, loss = 0.45158294\n",
      "Iteration 15, loss = 0.41904039\n",
      "Iteration 16, loss = 0.38805286\n",
      "Iteration 17, loss = 0.35855389\n",
      "Iteration 18, loss = 0.33167591\n",
      "Iteration 19, loss = 0.30602152\n",
      "Iteration 20, loss = 0.28221292\n",
      "Iteration 21, loss = 0.26012285\n",
      "Iteration 22, loss = 0.24019631\n",
      "Iteration 23, loss = 0.22234578\n",
      "Iteration 24, loss = 0.20553221\n",
      "Iteration 25, loss = 0.19045959\n",
      "Iteration 26, loss = 0.17663610\n",
      "Iteration 27, loss = 0.16433270\n",
      "Iteration 28, loss = 0.15328697\n",
      "Iteration 29, loss = 0.14295914\n",
      "Iteration 30, loss = 0.13391626\n",
      "Iteration 31, loss = 0.12576631\n",
      "Iteration 32, loss = 0.11823110\n",
      "Iteration 33, loss = 0.11155347\n",
      "Iteration 34, loss = 0.10546652\n",
      "Iteration 35, loss = 0.09996050\n",
      "Iteration 36, loss = 0.09497808\n",
      "Iteration 37, loss = 0.09038779\n",
      "Iteration 38, loss = 0.08639359\n",
      "Iteration 39, loss = 0.08269361\n",
      "Iteration 40, loss = 0.07930137\n",
      "Iteration 41, loss = 0.07625465\n",
      "Iteration 42, loss = 0.07352960\n",
      "Iteration 43, loss = 0.07100761\n",
      "Iteration 44, loss = 0.06855024\n",
      "Iteration 45, loss = 0.06639092\n",
      "Iteration 46, loss = 0.06437964\n",
      "Iteration 47, loss = 0.06257363\n",
      "Iteration 48, loss = 0.06094239\n",
      "Iteration 49, loss = 0.05930952\n",
      "Iteration 50, loss = 0.05787545\n",
      "Iteration 51, loss = 0.05657078\n",
      "Iteration 52, loss = 0.05540912\n",
      "Iteration 53, loss = 0.05427132\n",
      "Iteration 54, loss = 0.05316038\n",
      "Iteration 55, loss = 0.05215070\n",
      "Iteration 56, loss = 0.05123685\n",
      "Iteration 57, loss = 0.05034270\n",
      "Iteration 58, loss = 0.04966185\n",
      "Iteration 59, loss = 0.04877864\n",
      "Iteration 60, loss = 0.04822930\n",
      "Iteration 61, loss = 0.04738241\n",
      "Iteration 62, loss = 0.04675604\n",
      "Iteration 63, loss = 0.04611273\n",
      "Iteration 64, loss = 0.04553231\n",
      "Iteration 65, loss = 0.04507564\n",
      "Iteration 66, loss = 0.04454410\n",
      "Iteration 67, loss = 0.04403038\n",
      "Iteration 68, loss = 0.04356382\n",
      "Iteration 69, loss = 0.04327198\n",
      "Iteration 70, loss = 0.04280742\n",
      "Iteration 71, loss = 0.04235803\n",
      "Iteration 72, loss = 0.04201621\n",
      "Iteration 73, loss = 0.04163324\n",
      "Iteration 74, loss = 0.04132089\n",
      "Iteration 75, loss = 0.04099014\n",
      "Iteration 76, loss = 0.04063901\n",
      "Iteration 77, loss = 0.04035561\n",
      "Iteration 78, loss = 0.04007558\n",
      "Iteration 79, loss = 0.03986174\n",
      "Iteration 80, loss = 0.03961066\n",
      "Iteration 81, loss = 0.03931612\n",
      "Iteration 82, loss = 0.03910325\n",
      "Iteration 83, loss = 0.03893914\n",
      "Iteration 84, loss = 0.03886161\n",
      "Iteration 85, loss = 0.03858593\n",
      "Iteration 86, loss = 0.03838007\n",
      "Iteration 87, loss = 0.03816299\n",
      "Iteration 88, loss = 0.03786153\n",
      "Iteration 89, loss = 0.03766855\n",
      "Iteration 90, loss = 0.03750172\n",
      "Iteration 91, loss = 0.03734141\n",
      "Iteration 92, loss = 0.03718799\n",
      "Iteration 93, loss = 0.03711797\n",
      "Iteration 94, loss = 0.03694801\n",
      "Iteration 95, loss = 0.03674811\n",
      "Iteration 96, loss = 0.03673330\n",
      "Iteration 97, loss = 0.03651321\n",
      "Iteration 98, loss = 0.03634155\n",
      "Iteration 99, loss = 0.03638290\n",
      "Iteration 100, loss = 0.03608401\n",
      "Iteration 101, loss = 0.03597824\n",
      "Iteration 102, loss = 0.03587768\n",
      "Iteration 103, loss = 0.03574545\n",
      "Iteration 104, loss = 0.03568511\n",
      "Iteration 105, loss = 0.03554179\n",
      "Iteration 106, loss = 0.03543552\n",
      "Iteration 107, loss = 0.03540502\n",
      "Iteration 108, loss = 0.03526077\n",
      "Iteration 109, loss = 0.03517948\n",
      "Iteration 110, loss = 0.03519949\n",
      "Iteration 111, loss = 0.03492895\n",
      "Iteration 112, loss = 0.03512127\n",
      "Iteration 113, loss = 0.03483724\n",
      "Iteration 114, loss = 0.03478294\n",
      "Iteration 115, loss = 0.03467517\n",
      "Iteration 116, loss = 0.03459778\n",
      "Iteration 117, loss = 0.03453443\n",
      "Iteration 118, loss = 0.03456834\n",
      "Iteration 119, loss = 0.03437176\n",
      "Iteration 120, loss = 0.03428304\n",
      "Iteration 121, loss = 0.03425578\n",
      "Iteration 122, loss = 0.03424357\n",
      "Iteration 123, loss = 0.03423125\n",
      "Iteration 124, loss = 0.03404699\n",
      "Iteration 125, loss = 0.03409895\n",
      "Iteration 126, loss = 0.03403645\n",
      "Iteration 127, loss = 0.03396605\n",
      "Iteration 128, loss = 0.03384148\n",
      "Iteration 129, loss = 0.03385371\n",
      "Iteration 130, loss = 0.03390911\n",
      "Iteration 131, loss = 0.03368356\n",
      "Iteration 132, loss = 0.03351846\n",
      "Iteration 133, loss = 0.03356263\n",
      "Iteration 134, loss = 0.03367301\n",
      "Iteration 135, loss = 0.03370319\n",
      "Iteration 136, loss = 0.03356690\n",
      "Iteration 137, loss = 0.03350622\n",
      "Iteration 138, loss = 0.03345755\n",
      "Iteration 139, loss = 0.03332282\n",
      "Iteration 140, loss = 0.03331614\n",
      "Iteration 141, loss = 0.03317844\n",
      "Iteration 142, loss = 0.03344175\n",
      "Iteration 143, loss = 0.03322487\n",
      "Iteration 144, loss = 0.03311345\n",
      "Iteration 145, loss = 0.03309512\n",
      "Iteration 146, loss = 0.03306494\n",
      "Iteration 147, loss = 0.03298356\n",
      "Iteration 148, loss = 0.03305497\n",
      "Iteration 149, loss = 0.03287995\n",
      "Iteration 150, loss = 0.03283977\n",
      "Iteration 151, loss = 0.03286788\n",
      "Iteration 152, loss = 0.03280605\n",
      "Iteration 153, loss = 0.03282053\n",
      "Iteration 154, loss = 0.03277610\n",
      "Iteration 155, loss = 0.03271142\n",
      "Iteration 156, loss = 0.03271177\n",
      "Iteration 157, loss = 0.03270964\n",
      "Iteration 158, loss = 0.03263417\n",
      "Iteration 159, loss = 0.03259462\n",
      "Iteration 160, loss = 0.03256301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.02745325\n",
      "Iteration 2, loss = 0.97789903\n",
      "Iteration 3, loss = 0.93208994\n",
      "Iteration 4, loss = 0.88492990\n",
      "Iteration 5, loss = 0.83481881\n",
      "Iteration 6, loss = 0.78257692\n",
      "Iteration 7, loss = 0.73073354\n",
      "Iteration 8, loss = 0.68110108\n",
      "Iteration 9, loss = 0.63489090\n",
      "Iteration 10, loss = 0.59303917\n",
      "Iteration 11, loss = 0.55339545\n",
      "Iteration 12, loss = 0.51658876\n",
      "Iteration 13, loss = 0.48087273\n",
      "Iteration 14, loss = 0.44643838\n",
      "Iteration 15, loss = 0.41379094\n",
      "Iteration 16, loss = 0.38203056\n",
      "Iteration 17, loss = 0.35216914\n",
      "Iteration 18, loss = 0.32431510\n",
      "Iteration 19, loss = 0.29814614\n",
      "Iteration 20, loss = 0.27393440\n",
      "Iteration 21, loss = 0.25254717\n",
      "Iteration 22, loss = 0.23259875\n",
      "Iteration 23, loss = 0.21497193\n",
      "Iteration 24, loss = 0.19817323\n",
      "Iteration 25, loss = 0.18328830\n",
      "Iteration 26, loss = 0.17033128\n",
      "Iteration 27, loss = 0.15822644\n",
      "Iteration 28, loss = 0.14724094\n",
      "Iteration 29, loss = 0.13743405\n",
      "Iteration 30, loss = 0.12850322\n",
      "Iteration 31, loss = 0.12055709\n",
      "Iteration 32, loss = 0.11325951\n",
      "Iteration 33, loss = 0.10678081\n",
      "Iteration 34, loss = 0.10091633\n",
      "Iteration 35, loss = 0.09545790\n",
      "Iteration 36, loss = 0.09058813\n",
      "Iteration 37, loss = 0.08619765\n",
      "Iteration 38, loss = 0.08228645\n",
      "Iteration 39, loss = 0.07868794\n",
      "Iteration 40, loss = 0.07540021\n",
      "Iteration 41, loss = 0.07246930\n",
      "Iteration 42, loss = 0.06977334\n",
      "Iteration 43, loss = 0.06731562\n",
      "Iteration 44, loss = 0.06504584\n",
      "Iteration 45, loss = 0.06297416\n",
      "Iteration 46, loss = 0.06099691\n",
      "Iteration 47, loss = 0.05923321\n",
      "Iteration 48, loss = 0.05765055\n",
      "Iteration 49, loss = 0.05622116\n",
      "Iteration 50, loss = 0.05481647\n",
      "Iteration 51, loss = 0.05359269\n",
      "Iteration 52, loss = 0.05242420\n",
      "Iteration 53, loss = 0.05135499\n",
      "Iteration 54, loss = 0.05035813\n",
      "Iteration 55, loss = 0.04936345\n",
      "Iteration 56, loss = 0.04846053\n",
      "Iteration 57, loss = 0.04773191\n",
      "Iteration 58, loss = 0.04683988\n",
      "Iteration 59, loss = 0.04614941\n",
      "Iteration 60, loss = 0.04549638\n",
      "Iteration 61, loss = 0.04483332\n",
      "Iteration 62, loss = 0.04422654\n",
      "Iteration 63, loss = 0.04358288\n",
      "Iteration 64, loss = 0.04303937\n",
      "Iteration 65, loss = 0.04252065\n",
      "Iteration 66, loss = 0.04198444\n",
      "Iteration 67, loss = 0.04151936\n",
      "Iteration 68, loss = 0.04134193\n",
      "Iteration 69, loss = 0.04072207\n",
      "Iteration 70, loss = 0.04027747\n",
      "Iteration 71, loss = 0.04001453\n",
      "Iteration 72, loss = 0.03966029\n",
      "Iteration 73, loss = 0.03932125\n",
      "Iteration 74, loss = 0.03894911\n",
      "Iteration 75, loss = 0.03860303\n",
      "Iteration 76, loss = 0.03826822\n",
      "Iteration 77, loss = 0.03805504\n",
      "Iteration 78, loss = 0.03776237\n",
      "Iteration 79, loss = 0.03750765\n",
      "Iteration 80, loss = 0.03723447\n",
      "Iteration 81, loss = 0.03693460\n",
      "Iteration 82, loss = 0.03671872\n",
      "Iteration 83, loss = 0.03645670\n",
      "Iteration 84, loss = 0.03625571\n",
      "Iteration 85, loss = 0.03607783\n",
      "Iteration 86, loss = 0.03589048\n",
      "Iteration 87, loss = 0.03581206\n",
      "Iteration 88, loss = 0.03564133\n",
      "Iteration 89, loss = 0.03541374\n",
      "Iteration 90, loss = 0.03527275\n",
      "Iteration 91, loss = 0.03502929\n",
      "Iteration 92, loss = 0.03483645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 93, loss = 0.03478960\n",
      "Iteration 94, loss = 0.03458251\n",
      "Iteration 95, loss = 0.03453444\n",
      "Iteration 96, loss = 0.03447960\n",
      "Iteration 97, loss = 0.03416531\n",
      "Iteration 98, loss = 0.03400601\n",
      "Iteration 99, loss = 0.03389517\n",
      "Iteration 100, loss = 0.03383089\n",
      "Iteration 101, loss = 0.03377264\n",
      "Iteration 102, loss = 0.03356856\n",
      "Iteration 103, loss = 0.03344694\n",
      "Iteration 104, loss = 0.03330890\n",
      "Iteration 105, loss = 0.03325990\n",
      "Iteration 106, loss = 0.03319673\n",
      "Iteration 107, loss = 0.03295817\n",
      "Iteration 108, loss = 0.03292838\n",
      "Iteration 109, loss = 0.03283988\n",
      "Iteration 110, loss = 0.03296521\n",
      "Iteration 111, loss = 0.03268641\n",
      "Iteration 112, loss = 0.03271108\n",
      "Iteration 113, loss = 0.03252735\n",
      "Iteration 114, loss = 0.03245646\n",
      "Iteration 115, loss = 0.03238734\n",
      "Iteration 116, loss = 0.03234641\n",
      "Iteration 117, loss = 0.03227725\n",
      "Iteration 118, loss = 0.03213803\n",
      "Iteration 119, loss = 0.03205529\n",
      "Iteration 120, loss = 0.03193720\n",
      "Iteration 121, loss = 0.03186591\n",
      "Iteration 122, loss = 0.03177427\n",
      "Iteration 123, loss = 0.03171511\n",
      "Iteration 124, loss = 0.03174126\n",
      "Iteration 125, loss = 0.03166878\n",
      "Iteration 126, loss = 0.03152753\n",
      "Iteration 127, loss = 0.03149142\n",
      "Iteration 128, loss = 0.03143203\n",
      "Iteration 129, loss = 0.03141136\n",
      "Iteration 130, loss = 0.03130984\n",
      "Iteration 131, loss = 0.03127651\n",
      "Iteration 132, loss = 0.03125329\n",
      "Iteration 133, loss = 0.03122046\n",
      "Iteration 134, loss = 0.03121015\n",
      "Iteration 135, loss = 0.03109599\n",
      "Iteration 136, loss = 0.03102593\n",
      "Iteration 137, loss = 0.03095583\n",
      "Iteration 138, loss = 0.03092767\n",
      "Iteration 139, loss = 0.03088004\n",
      "Iteration 140, loss = 0.03077595\n",
      "Iteration 141, loss = 0.03074559\n",
      "Iteration 142, loss = 0.03079814\n",
      "Iteration 143, loss = 0.03069469\n",
      "Iteration 144, loss = 0.03063400\n",
      "Iteration 145, loss = 0.03058843\n",
      "Iteration 146, loss = 0.03052092\n",
      "Iteration 147, loss = 0.03056182\n",
      "Iteration 148, loss = 0.03055080\n",
      "Iteration 149, loss = 0.03040758\n",
      "Iteration 150, loss = 0.03042135\n",
      "Iteration 151, loss = 0.03037760\n",
      "Iteration 152, loss = 0.03032313\n",
      "Iteration 153, loss = 0.03041900\n",
      "Iteration 154, loss = 0.03033671\n",
      "Iteration 155, loss = 0.03025015\n",
      "Iteration 156, loss = 0.03026152\n",
      "Iteration 157, loss = 0.03028953\n",
      "Iteration 158, loss = 0.03016372\n",
      "Iteration 159, loss = 0.03016239\n",
      "Iteration 160, loss = 0.03006445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07165911\n",
      "Iteration 2, loss = 1.01476188\n",
      "Iteration 3, loss = 0.95982694\n",
      "Iteration 4, loss = 0.90163233\n",
      "Iteration 5, loss = 0.84097523\n",
      "Iteration 6, loss = 0.78133924\n",
      "Iteration 7, loss = 0.72377723\n",
      "Iteration 8, loss = 0.67200056\n",
      "Iteration 9, loss = 0.62602604\n",
      "Iteration 10, loss = 0.58489198\n",
      "Iteration 11, loss = 0.54767468\n",
      "Iteration 12, loss = 0.51297223\n",
      "Iteration 13, loss = 0.47962195\n",
      "Iteration 14, loss = 0.44738404\n",
      "Iteration 15, loss = 0.41680195\n",
      "Iteration 16, loss = 0.38652298\n",
      "Iteration 17, loss = 0.35758239\n",
      "Iteration 18, loss = 0.33045357\n",
      "Iteration 19, loss = 0.30522149\n",
      "Iteration 20, loss = 0.28177196\n",
      "Iteration 21, loss = 0.25996988\n",
      "Iteration 22, loss = 0.24049996\n",
      "Iteration 23, loss = 0.22260187\n",
      "Iteration 24, loss = 0.20584073\n",
      "Iteration 25, loss = 0.19142171\n",
      "Iteration 26, loss = 0.17789866\n",
      "Iteration 27, loss = 0.16586220\n",
      "Iteration 28, loss = 0.15477667\n",
      "Iteration 29, loss = 0.14464150\n",
      "Iteration 30, loss = 0.13542512\n",
      "Iteration 31, loss = 0.12724981\n",
      "Iteration 32, loss = 0.11997507\n",
      "Iteration 33, loss = 0.11304159\n",
      "Iteration 34, loss = 0.10699349\n",
      "Iteration 35, loss = 0.10158954\n",
      "Iteration 36, loss = 0.09644623\n",
      "Iteration 37, loss = 0.09191805\n",
      "Iteration 38, loss = 0.08777304\n",
      "Iteration 39, loss = 0.08402419\n",
      "Iteration 40, loss = 0.08059687\n",
      "Iteration 41, loss = 0.07750828\n",
      "Iteration 42, loss = 0.07462467\n",
      "Iteration 43, loss = 0.07198413\n",
      "Iteration 44, loss = 0.06953634\n",
      "Iteration 45, loss = 0.06738112\n",
      "Iteration 46, loss = 0.06539037\n",
      "Iteration 47, loss = 0.06355828\n",
      "Iteration 48, loss = 0.06180823\n",
      "Iteration 49, loss = 0.06022044\n",
      "Iteration 50, loss = 0.05883052\n",
      "Iteration 51, loss = 0.05743746\n",
      "Iteration 52, loss = 0.05619797\n",
      "Iteration 53, loss = 0.05502866\n",
      "Iteration 54, loss = 0.05401795\n",
      "Iteration 55, loss = 0.05297662\n",
      "Iteration 56, loss = 0.05208055\n",
      "Iteration 57, loss = 0.05123393\n",
      "Iteration 58, loss = 0.05046731\n",
      "Iteration 59, loss = 0.04966199\n",
      "Iteration 60, loss = 0.04891869\n",
      "Iteration 61, loss = 0.04829227\n",
      "Iteration 62, loss = 0.04758559\n",
      "Iteration 63, loss = 0.04702608\n",
      "Iteration 64, loss = 0.04647269\n",
      "Iteration 65, loss = 0.04589728\n",
      "Iteration 66, loss = 0.04543115\n",
      "Iteration 67, loss = 0.04496618\n",
      "Iteration 68, loss = 0.04449801\n",
      "Iteration 69, loss = 0.04401300\n",
      "Iteration 70, loss = 0.04362942\n",
      "Iteration 71, loss = 0.04326225\n",
      "Iteration 72, loss = 0.04290897\n",
      "Iteration 73, loss = 0.04263363\n",
      "Iteration 74, loss = 0.04230874\n",
      "Iteration 75, loss = 0.04195859\n",
      "Iteration 76, loss = 0.04162178\n",
      "Iteration 77, loss = 0.04126958\n",
      "Iteration 78, loss = 0.04098922\n",
      "Iteration 79, loss = 0.04076061\n",
      "Iteration 80, loss = 0.04048394\n",
      "Iteration 81, loss = 0.04024298\n",
      "Iteration 82, loss = 0.04002166\n",
      "Iteration 83, loss = 0.03976274\n",
      "Iteration 84, loss = 0.03956841\n",
      "Iteration 85, loss = 0.03933617\n",
      "Iteration 86, loss = 0.03918924\n",
      "Iteration 87, loss = 0.03898853\n",
      "Iteration 88, loss = 0.03883162\n",
      "Iteration 89, loss = 0.03859876\n",
      "Iteration 90, loss = 0.03860827\n",
      "Iteration 91, loss = 0.03840081\n",
      "Iteration 92, loss = 0.03816986\n",
      "Iteration 93, loss = 0.03798637\n",
      "Iteration 94, loss = 0.03781313\n",
      "Iteration 95, loss = 0.03783541\n",
      "Iteration 96, loss = 0.03770005\n",
      "Iteration 97, loss = 0.03743584\n",
      "Iteration 98, loss = 0.03730410\n",
      "Iteration 99, loss = 0.03717333\n",
      "Iteration 100, loss = 0.03700914\n",
      "Iteration 101, loss = 0.03705923\n",
      "Iteration 102, loss = 0.03678859\n",
      "Iteration 103, loss = 0.03669899\n",
      "Iteration 104, loss = 0.03660858\n",
      "Iteration 105, loss = 0.03649226\n",
      "Iteration 106, loss = 0.03636259\n",
      "Iteration 107, loss = 0.03635797\n",
      "Iteration 108, loss = 0.03629047\n",
      "Iteration 109, loss = 0.03611887\n",
      "Iteration 110, loss = 0.03611048\n",
      "Iteration 111, loss = 0.03597226\n",
      "Iteration 112, loss = 0.03588507\n",
      "Iteration 113, loss = 0.03580375\n",
      "Iteration 114, loss = 0.03570427\n",
      "Iteration 115, loss = 0.03571599\n",
      "Iteration 116, loss = 0.03565846\n",
      "Iteration 117, loss = 0.03562607\n",
      "Iteration 118, loss = 0.03551789\n",
      "Iteration 119, loss = 0.03538811\n",
      "Iteration 120, loss = 0.03525460\n",
      "Iteration 121, loss = 0.03522143\n",
      "Iteration 122, loss = 0.03519986\n",
      "Iteration 123, loss = 0.03522644\n",
      "Iteration 124, loss = 0.03513936\n",
      "Iteration 125, loss = 0.03498372\n",
      "Iteration 126, loss = 0.03497286\n",
      "Iteration 127, loss = 0.03496623\n",
      "Iteration 128, loss = 0.03495157\n",
      "Iteration 129, loss = 0.03495104\n",
      "Iteration 130, loss = 0.03486945\n",
      "Iteration 131, loss = 0.03468158\n",
      "Iteration 132, loss = 0.03460837\n",
      "Iteration 133, loss = 0.03453692\n",
      "Iteration 134, loss = 0.03454569\n",
      "Iteration 135, loss = 0.03450545\n",
      "Iteration 136, loss = 0.03438594\n",
      "Iteration 137, loss = 0.03454823\n",
      "Iteration 138, loss = 0.03427192\n",
      "Iteration 139, loss = 0.03424580\n",
      "Iteration 140, loss = 0.03422382\n",
      "Iteration 141, loss = 0.03417711\n",
      "Iteration 142, loss = 0.03420582\n",
      "Iteration 143, loss = 0.03411251\n",
      "Iteration 144, loss = 0.03406595\n",
      "Iteration 145, loss = 0.03405011\n",
      "Iteration 146, loss = 0.03416810\n",
      "Iteration 147, loss = 0.03395778\n",
      "Iteration 148, loss = 0.03390034\n",
      "Iteration 149, loss = 0.03392267\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06388812\n",
      "Iteration 2, loss = 0.99767329\n",
      "Iteration 3, loss = 0.93619827\n",
      "Iteration 4, loss = 0.87427746\n",
      "Iteration 5, loss = 0.81137558\n",
      "Iteration 6, loss = 0.75000490\n",
      "Iteration 7, loss = 0.69436922\n",
      "Iteration 8, loss = 0.64287678\n",
      "Iteration 9, loss = 0.59771404\n",
      "Iteration 10, loss = 0.55656305\n",
      "Iteration 11, loss = 0.51822728\n",
      "Iteration 12, loss = 0.48166322\n",
      "Iteration 13, loss = 0.44715063\n",
      "Iteration 14, loss = 0.41344067\n",
      "Iteration 15, loss = 0.38195107\n",
      "Iteration 16, loss = 0.35192788\n",
      "Iteration 17, loss = 0.32409671\n",
      "Iteration 18, loss = 0.29803461\n",
      "Iteration 19, loss = 0.27497781\n",
      "Iteration 20, loss = 0.25285793\n",
      "Iteration 21, loss = 0.23323898\n",
      "Iteration 22, loss = 0.21580161\n",
      "Iteration 23, loss = 0.19976433\n",
      "Iteration 24, loss = 0.18521173\n",
      "Iteration 25, loss = 0.17232425\n",
      "Iteration 26, loss = 0.16036993\n",
      "Iteration 27, loss = 0.14981435\n",
      "Iteration 28, loss = 0.14019522\n",
      "Iteration 29, loss = 0.13152836\n",
      "Iteration 30, loss = 0.12356667\n",
      "Iteration 31, loss = 0.11646596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.11009471\n",
      "Iteration 33, loss = 0.10421010\n",
      "Iteration 34, loss = 0.09889072\n",
      "Iteration 35, loss = 0.09418078\n",
      "Iteration 36, loss = 0.08994771\n",
      "Iteration 37, loss = 0.08608197\n",
      "Iteration 38, loss = 0.08247080\n",
      "Iteration 39, loss = 0.07923439\n",
      "Iteration 40, loss = 0.07628090\n",
      "Iteration 41, loss = 0.07361502\n",
      "Iteration 42, loss = 0.07110171\n",
      "Iteration 43, loss = 0.06891151\n",
      "Iteration 44, loss = 0.06681072\n",
      "Iteration 45, loss = 0.06489220\n",
      "Iteration 46, loss = 0.06312314\n",
      "Iteration 47, loss = 0.06153101\n",
      "Iteration 48, loss = 0.06006927\n",
      "Iteration 49, loss = 0.05856603\n",
      "Iteration 50, loss = 0.05733246\n",
      "Iteration 51, loss = 0.05615911\n",
      "Iteration 52, loss = 0.05502148\n",
      "Iteration 53, loss = 0.05397744\n",
      "Iteration 54, loss = 0.05306228\n",
      "Iteration 55, loss = 0.05211931\n",
      "Iteration 56, loss = 0.05126838\n",
      "Iteration 57, loss = 0.05055383\n",
      "Iteration 58, loss = 0.04971749\n",
      "Iteration 59, loss = 0.04902880\n",
      "Iteration 60, loss = 0.04835028\n",
      "Iteration 61, loss = 0.04780376\n",
      "Iteration 62, loss = 0.04718294\n",
      "Iteration 63, loss = 0.04661495\n",
      "Iteration 64, loss = 0.04611365\n",
      "Iteration 65, loss = 0.04563614\n",
      "Iteration 66, loss = 0.04515956\n",
      "Iteration 67, loss = 0.04471482\n",
      "Iteration 68, loss = 0.04435938\n",
      "Iteration 69, loss = 0.04387685\n",
      "Iteration 70, loss = 0.04357723\n",
      "Iteration 71, loss = 0.04317840\n",
      "Iteration 72, loss = 0.04285567\n",
      "Iteration 73, loss = 0.04250983\n",
      "Iteration 74, loss = 0.04220943\n",
      "Iteration 75, loss = 0.04199341\n",
      "Iteration 76, loss = 0.04163423\n",
      "Iteration 77, loss = 0.04131139\n",
      "Iteration 78, loss = 0.04107707\n",
      "Iteration 79, loss = 0.04079804\n",
      "Iteration 80, loss = 0.04058116\n",
      "Iteration 81, loss = 0.04044938\n",
      "Iteration 82, loss = 0.04024574\n",
      "Iteration 83, loss = 0.04002056\n",
      "Iteration 84, loss = 0.03974468\n",
      "Iteration 85, loss = 0.03954411\n",
      "Iteration 86, loss = 0.03931052\n",
      "Iteration 87, loss = 0.03918478\n",
      "Iteration 88, loss = 0.03895439\n",
      "Iteration 89, loss = 0.03879291\n",
      "Iteration 90, loss = 0.03870731\n",
      "Iteration 91, loss = 0.03847859\n",
      "Iteration 92, loss = 0.03832655\n",
      "Iteration 93, loss = 0.03824721\n",
      "Iteration 94, loss = 0.03806362\n",
      "Iteration 95, loss = 0.03787989\n",
      "Iteration 96, loss = 0.03778753\n",
      "Iteration 97, loss = 0.03763361\n",
      "Iteration 98, loss = 0.03752211\n",
      "Iteration 99, loss = 0.03740601\n",
      "Iteration 100, loss = 0.03740830\n",
      "Iteration 101, loss = 0.03718606\n",
      "Iteration 102, loss = 0.03707405\n",
      "Iteration 103, loss = 0.03692989\n",
      "Iteration 104, loss = 0.03703658\n",
      "Iteration 105, loss = 0.03677524\n",
      "Iteration 106, loss = 0.03671657\n",
      "Iteration 107, loss = 0.03656650\n",
      "Iteration 108, loss = 0.03651662\n",
      "Iteration 109, loss = 0.03644145\n",
      "Iteration 110, loss = 0.03629685\n",
      "Iteration 111, loss = 0.03635289\n",
      "Iteration 112, loss = 0.03612100\n",
      "Iteration 113, loss = 0.03605336\n",
      "Iteration 114, loss = 0.03592663\n",
      "Iteration 115, loss = 0.03591597\n",
      "Iteration 116, loss = 0.03579550\n",
      "Iteration 117, loss = 0.03574110\n",
      "Iteration 118, loss = 0.03569518\n",
      "Iteration 119, loss = 0.03566214\n",
      "Iteration 120, loss = 0.03552055\n",
      "Iteration 121, loss = 0.03550902\n",
      "Iteration 122, loss = 0.03551737\n",
      "Iteration 123, loss = 0.03534432\n",
      "Iteration 124, loss = 0.03528475\n",
      "Iteration 125, loss = 0.03521152\n",
      "Iteration 126, loss = 0.03520733\n",
      "Iteration 127, loss = 0.03511789\n",
      "Iteration 128, loss = 0.03509573\n",
      "Iteration 129, loss = 0.03502588\n",
      "Iteration 130, loss = 0.03491717\n",
      "Iteration 131, loss = 0.03511594\n",
      "Iteration 132, loss = 0.03476882\n",
      "Iteration 133, loss = 0.03473783\n",
      "Iteration 134, loss = 0.03480556\n",
      "Iteration 135, loss = 0.03471879\n",
      "Iteration 136, loss = 0.03467553\n",
      "Iteration 137, loss = 0.03458747\n",
      "Iteration 138, loss = 0.03454291\n",
      "Iteration 139, loss = 0.03455139\n",
      "Iteration 140, loss = 0.03442883\n",
      "Iteration 141, loss = 0.03439927\n",
      "Iteration 142, loss = 0.03440080\n",
      "Iteration 143, loss = 0.03435665\n",
      "Iteration 144, loss = 0.03429137\n",
      "Iteration 145, loss = 0.03422503\n",
      "Iteration 146, loss = 0.03423405\n",
      "Iteration 147, loss = 0.03422486\n",
      "Iteration 148, loss = 0.03414676\n",
      "Iteration 149, loss = 0.03409685\n",
      "Iteration 150, loss = 0.03408161\n",
      "Iteration 151, loss = 0.03408018\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# call the train model function and return the fitted model\n",
    "model1 = train_model(classifier, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions from fitted model\n",
    "predicts1 = model1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.986472\n",
      "Precision: 0.993914\n",
      "Recall: 0.973882\n",
      "F1 score: 0.983567\n"
     ]
    }
   ],
   "source": [
    "# return measurement calculations\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import precision_score \n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import confusion_matrix  \n",
    "\n",
    "# accuracy: (tp + tn) / (p + n) \n",
    "accuracy = accuracy_score(y, predicts1) \n",
    "print('Accuracy: %f' % accuracy) \n",
    "\n",
    "# precision tp / (tp + fp) \n",
    "precision = precision_score(y, predicts1, average = 'macro') \n",
    "print('Precision: %f' % precision) \n",
    "\n",
    "# recall: tp / (tp + fn) \n",
    "recall = recall_score(y, predicts1, average = 'macro') \n",
    "print('Recall: %f' % recall) \n",
    "\n",
    "# f1: 2 tp / (2 tp + fp + fn) \n",
    "f1 = f1_score(y, predicts1, average = 'macro') \n",
    "print('F1 score: %f' % f1)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix \n",
    "matrix = confusion_matrix(y, predicts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 41   0   1]\n",
      " [  0 208  12]\n",
      " [  0   0 699]]\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** The model performance is 100 based off only 750 samples sent to the nueral network. This was due to the amount of data processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network Classifier with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf-idf feature matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features = 3000)\n",
    "X = tfidf.fit_transform(df['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the TfidfVectorizer turned the feature variable to a sparse matrix, which causes problems in the model.\n",
    "# solve the error by converting the sparse matrix to a dense matrix\n",
    "X = X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(961, 3000)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFeatures = 3000\n",
    "nClasses = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model \n",
    "def build_network():\n",
    "    \"\"\"\n",
    "    Create a function that returns a compiled neural network\n",
    "    \"\"\"\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(500, activation = 'relu', input_shape =(nFeatures,)))\n",
    "    nn.add(Dense(150, activation = 'relu'))\n",
    "    nn.add(Dense(nClasses, activation = 'softmax'))\n",
    "    nn.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy']\n",
    "              )\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.0545 - accuracy: 0.5194 - val_loss: 0.8375 - val_accuracy: 1.0000\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.9017 - accuracy: 0.5925 - val_loss: 0.5553 - val_accuracy: 1.0000\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.7861 - accuracy: 0.5925 - val_loss: 0.3906 - val_accuracy: 1.0000\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.7056 - accuracy: 0.5925 - val_loss: 0.4829 - val_accuracy: 1.0000\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6253 - accuracy: 0.7947 - val_loss: 0.5009 - val_accuracy: 0.9748\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5112 - accuracy: 0.8787 - val_loss: 0.3912 - val_accuracy: 0.9717\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3988 - accuracy: 0.9005 - val_loss: 0.4188 - val_accuracy: 0.9182\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3049 - accuracy: 0.9051 - val_loss: 0.3770 - val_accuracy: 0.8899\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.2290 - accuracy: 0.9067 - val_loss: 0.3449 - val_accuracy: 0.8774\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1800 - accuracy: 0.9098 - val_loss: 0.3397 - val_accuracy: 0.8585\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1505 - accuracy: 0.9425 - val_loss: 0.3197 - val_accuracy: 0.8679\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.1296 - accuracy: 0.9642 - val_loss: 0.3562 - val_accuracy: 0.8522\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1130 - accuracy: 0.9736 - val_loss: 0.3709 - val_accuracy: 0.8491\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0985 - accuracy: 0.9751 - val_loss: 0.3681 - val_accuracy: 0.8522\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0844 - accuracy: 0.9751 - val_loss: 0.3740 - val_accuracy: 0.8522\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0728 - accuracy: 0.9751 - val_loss: 0.3633 - val_accuracy: 0.8585\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0674 - accuracy: 0.9751 - val_loss: 0.2626 - val_accuracy: 0.9057\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0749 - accuracy: 0.9751 - val_loss: 0.3634 - val_accuracy: 0.8648\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0653 - accuracy: 0.9705 - val_loss: 0.5929 - val_accuracy: 0.7547\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0592 - accuracy: 0.9720 - val_loss: 0.5698 - val_accuracy: 0.7610\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0536 - accuracy: 0.9720 - val_loss: 0.4000 - val_accuracy: 0.8522\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0537 - accuracy: 0.9751 - val_loss: 0.3548 - val_accuracy: 0.8648\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0535 - accuracy: 0.9751 - val_loss: 0.4046 - val_accuracy: 0.8428\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0505 - accuracy: 0.9751 - val_loss: 0.4609 - val_accuracy: 0.8113\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0511 - accuracy: 0.9720 - val_loss: 0.4605 - val_accuracy: 0.8113\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0500 - accuracy: 0.9751 - val_loss: 0.3817 - val_accuracy: 0.8522\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0584 - accuracy: 0.9751 - val_loss: 0.2350 - val_accuracy: 0.9025\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0586 - accuracy: 0.9751 - val_loss: 0.3514 - val_accuracy: 0.8616\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0517 - accuracy: 0.9720 - val_loss: 0.4324 - val_accuracy: 0.8082\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0523 - accuracy: 0.9673 - val_loss: 0.4441 - val_accuracy: 0.8050\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0503 - accuracy: 0.9720 - val_loss: 0.4125 - val_accuracy: 0.8396\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0491 - accuracy: 0.9751 - val_loss: 0.4508 - val_accuracy: 0.8239\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0496 - accuracy: 0.9751 - val_loss: 0.4878 - val_accuracy: 0.8050\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0491 - accuracy: 0.9751 - val_loss: 0.4534 - val_accuracy: 0.8270\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0487 - accuracy: 0.9751 - val_loss: 0.5848 - val_accuracy: 0.7579\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0622 - accuracy: 0.9596 - val_loss: 0.9697 - val_accuracy: 0.6321\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0775 - accuracy: 0.9518 - val_loss: 0.6496 - val_accuracy: 0.7358\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0664 - accuracy: 0.9673 - val_loss: 0.5628 - val_accuracy: 0.7673\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0683 - accuracy: 0.9642 - val_loss: 0.8779 - val_accuracy: 0.6824\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0918 - accuracy: 0.9565 - val_loss: 0.6209 - val_accuracy: 0.7327\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0657 - accuracy: 0.9596 - val_loss: 0.3036 - val_accuracy: 0.8868\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0561 - accuracy: 0.9751 - val_loss: 0.2365 - val_accuracy: 0.8994\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0559 - accuracy: 0.9751 - val_loss: 0.3160 - val_accuracy: 0.8774\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0505 - accuracy: 0.9751 - val_loss: 0.4374 - val_accuracy: 0.8239\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0488 - accuracy: 0.9751 - val_loss: 0.5193 - val_accuracy: 0.7987\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0498 - accuracy: 0.9705 - val_loss: 0.5618 - val_accuracy: 0.7830\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0487 - accuracy: 0.9751 - val_loss: 0.5169 - val_accuracy: 0.8019\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0480 - accuracy: 0.9751 - val_loss: 0.4948 - val_accuracy: 0.8050\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0483 - accuracy: 0.9751 - val_loss: 0.4782 - val_accuracy: 0.8145\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0479 - accuracy: 0.9751 - val_loss: 0.4201 - val_accuracy: 0.8302\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0502 - accuracy: 0.9751 - val_loss: 0.4158 - val_accuracy: 0.8302\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0503 - accuracy: 0.9751 - val_loss: 0.4658 - val_accuracy: 0.8208\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0498 - accuracy: 0.9751 - val_loss: 0.4994 - val_accuracy: 0.8082\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0491 - accuracy: 0.9751 - val_loss: 0.4996 - val_accuracy: 0.8082\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0490 - accuracy: 0.9751 - val_loss: 0.4964 - val_accuracy: 0.8113\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0484 - accuracy: 0.9751 - val_loss: 0.5118 - val_accuracy: 0.8082\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0480 - accuracy: 0.9751 - val_loss: 0.6026 - val_accuracy: 0.7484\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0607 - accuracy: 0.9658 - val_loss: 0.9297 - val_accuracy: 0.6761\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0767 - accuracy: 0.9658 - val_loss: 0.6165 - val_accuracy: 0.7547\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0534 - accuracy: 0.9720 - val_loss: 0.2993 - val_accuracy: 0.8868\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0557 - accuracy: 0.9751 - val_loss: 0.3412 - val_accuracy: 0.8774\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0541 - accuracy: 0.9720 - val_loss: 0.9823 - val_accuracy: 0.6478\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0778 - accuracy: 0.9642 - val_loss: 1.1073 - val_accuracy: 0.6132\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0704 - accuracy: 0.9642 - val_loss: 0.7934 - val_accuracy: 0.7264\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0576 - accuracy: 0.9720 - val_loss: 0.5682 - val_accuracy: 0.7642\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0573 - accuracy: 0.9736 - val_loss: 0.4560 - val_accuracy: 0.8208\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0550 - accuracy: 0.9736 - val_loss: 0.4488 - val_accuracy: 0.8208\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0520 - accuracy: 0.9736 - val_loss: 0.4642 - val_accuracy: 0.8208\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0498 - accuracy: 0.9736 - val_loss: 0.4910 - val_accuracy: 0.8082\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0489 - accuracy: 0.9736 - val_loss: 0.4846 - val_accuracy: 0.8145\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0484 - accuracy: 0.9736 - val_loss: 0.4597 - val_accuracy: 0.8239\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0486 - accuracy: 0.9751 - val_loss: 0.4320 - val_accuracy: 0.8365\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0479 - accuracy: 0.9751 - val_loss: 0.5214 - val_accuracy: 0.7956\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0554 - accuracy: 0.9689 - val_loss: 0.8057 - val_accuracy: 0.6981\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0562 - accuracy: 0.9705 - val_loss: 0.5302 - val_accuracy: 0.7830\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0609 - accuracy: 0.9736 - val_loss: 0.4477 - val_accuracy: 0.8239\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0639 - accuracy: 0.9736 - val_loss: 0.4693 - val_accuracy: 0.8145\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0589 - accuracy: 0.9736 - val_loss: 0.5114 - val_accuracy: 0.7956\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0540 - accuracy: 0.9736 - val_loss: 0.5565 - val_accuracy: 0.7767\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0508 - accuracy: 0.9736 - val_loss: 0.5230 - val_accuracy: 0.7893\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0504 - accuracy: 0.9736 - val_loss: 0.4785 - val_accuracy: 0.8050\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0502 - accuracy: 0.9736 - val_loss: 0.4782 - val_accuracy: 0.8050\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0495 - accuracy: 0.9736 - val_loss: 0.4666 - val_accuracy: 0.8145\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0493 - accuracy: 0.9736 - val_loss: 0.4820 - val_accuracy: 0.8050\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0487 - accuracy: 0.9751 - val_loss: 0.4400 - val_accuracy: 0.8365\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0506 - accuracy: 0.9751 - val_loss: 0.3229 - val_accuracy: 0.8805\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0618 - accuracy: 0.9751 - val_loss: 0.2987 - val_accuracy: 0.8836\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0663 - accuracy: 0.9751 - val_loss: 0.3550 - val_accuracy: 0.8679\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0609 - accuracy: 0.9751 - val_loss: 0.5287 - val_accuracy: 0.7987\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0513 - accuracy: 0.9720 - val_loss: 0.7165 - val_accuracy: 0.7296\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0502 - accuracy: 0.9736 - val_loss: 0.7881 - val_accuracy: 0.7138\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0501 - accuracy: 0.9736 - val_loss: 0.6793 - val_accuracy: 0.7453\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0479 - accuracy: 0.9736 - val_loss: 0.5881 - val_accuracy: 0.7830\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0476 - accuracy: 0.9736 - val_loss: 0.4650 - val_accuracy: 0.8239\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0523 - accuracy: 0.9751 - val_loss: 0.3105 - val_accuracy: 0.8836\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0618 - accuracy: 0.9751 - val_loss: 0.3290 - val_accuracy: 0.8774\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0587 - accuracy: 0.9751 - val_loss: 0.3999 - val_accuracy: 0.8585\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0520 - accuracy: 0.9751 - val_loss: 0.6011 - val_accuracy: 0.7767\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0560 - accuracy: 0.9596 - val_loss: 1.2115 - val_accuracy: 0.5943\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0707 - accuracy: 0.9642 - val_loss: 1.1772 - val_accuracy: 0.6069\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0636 - accuracy: 0.9658 - val_loss: 0.8284 - val_accuracy: 0.7170\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0558 - accuracy: 0.9736 - val_loss: 0.5592 - val_accuracy: 0.7925\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0537 - accuracy: 0.9736 - val_loss: 0.4510 - val_accuracy: 0.8365\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0531 - accuracy: 0.9736 - val_loss: 0.4241 - val_accuracy: 0.8396\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0511 - accuracy: 0.9736 - val_loss: 0.4147 - val_accuracy: 0.8428\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0515 - accuracy: 0.9736 - val_loss: 0.3667 - val_accuracy: 0.8679\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0520 - accuracy: 0.9751 - val_loss: 0.3970 - val_accuracy: 0.8553\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0495 - accuracy: 0.9736 - val_loss: 0.4481 - val_accuracy: 0.8333\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0481 - accuracy: 0.9736 - val_loss: 0.4954 - val_accuracy: 0.8302\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0479 - accuracy: 0.9736 - val_loss: 0.5081 - val_accuracy: 0.8270\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0473 - accuracy: 0.9736 - val_loss: 0.4772 - val_accuracy: 0.8396\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0472 - accuracy: 0.9751 - val_loss: 0.4475 - val_accuracy: 0.8396\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0476 - accuracy: 0.9751 - val_loss: 0.4377 - val_accuracy: 0.8396\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0475 - accuracy: 0.9751 - val_loss: 0.4525 - val_accuracy: 0.8396\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0477 - accuracy: 0.9751 - val_loss: 0.4979 - val_accuracy: 0.8333\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0472 - accuracy: 0.9751 - val_loss: 0.5798 - val_accuracy: 0.7956\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0570 - accuracy: 0.9627 - val_loss: 0.9632 - val_accuracy: 0.6824\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0647 - accuracy: 0.9658 - val_loss: 0.9225 - val_accuracy: 0.6855\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0577 - accuracy: 0.9720 - val_loss: 0.6909 - val_accuracy: 0.7579\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0531 - accuracy: 0.9736 - val_loss: 0.5244 - val_accuracy: 0.8208\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0520 - accuracy: 0.9736 - val_loss: 0.4647 - val_accuracy: 0.8333\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0514 - accuracy: 0.9736 - val_loss: 0.4574 - val_accuracy: 0.8365\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0494 - accuracy: 0.9736 - val_loss: 0.4781 - val_accuracy: 0.8302\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0484 - accuracy: 0.9736 - val_loss: 0.4889 - val_accuracy: 0.8302\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0480 - accuracy: 0.9736 - val_loss: 0.4752 - val_accuracy: 0.8302\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0477 - accuracy: 0.9736 - val_loss: 0.4234 - val_accuracy: 0.8491\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0483 - accuracy: 0.9751 - val_loss: 0.4092 - val_accuracy: 0.8491\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0490 - accuracy: 0.9751 - val_loss: 0.4149 - val_accuracy: 0.8491\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0487 - accuracy: 0.9751 - val_loss: 0.3858 - val_accuracy: 0.8679\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0511 - accuracy: 0.9751 - val_loss: 0.2772 - val_accuracy: 0.8899\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0555 - accuracy: 0.9751 - val_loss: 0.3596 - val_accuracy: 0.8774\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0563 - accuracy: 0.9642 - val_loss: 0.9364 - val_accuracy: 0.6792\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0694 - accuracy: 0.9658 - val_loss: 1.1140 - val_accuracy: 0.6352\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0672 - accuracy: 0.9658 - val_loss: 0.9094 - val_accuracy: 0.6792\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0552 - accuracy: 0.9673 - val_loss: 0.6764 - val_accuracy: 0.7579\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0511 - accuracy: 0.9736 - val_loss: 0.4743 - val_accuracy: 0.8333\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0572 - accuracy: 0.9720 - val_loss: 0.2483 - val_accuracy: 0.9057\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0653 - accuracy: 0.9751 - val_loss: 0.2340 - val_accuracy: 0.9088\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0622 - accuracy: 0.9751 - val_loss: 0.2965 - val_accuracy: 0.8931\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0565 - accuracy: 0.9751 - val_loss: 0.3739 - val_accuracy: 0.8648\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0522 - accuracy: 0.9751 - val_loss: 0.4863 - val_accuracy: 0.8239\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0493 - accuracy: 0.9751 - val_loss: 0.6191 - val_accuracy: 0.7862\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0492 - accuracy: 0.9705 - val_loss: 0.7259 - val_accuracy: 0.7327\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0501 - accuracy: 0.9673 - val_loss: 0.7073 - val_accuracy: 0.7453\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0492 - accuracy: 0.9673 - val_loss: 0.6484 - val_accuracy: 0.7610\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0480 - accuracy: 0.9798 - val_loss: 0.5734 - val_accuracy: 0.7956\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0488 - accuracy: 0.9751 - val_loss: 0.4358 - val_accuracy: 0.8459\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0539 - accuracy: 0.9751 - val_loss: 0.4259 - val_accuracy: 0.8459\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0542 - accuracy: 0.9751 - val_loss: 0.4961 - val_accuracy: 0.8239\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0502 - accuracy: 0.9751 - val_loss: 0.5835 - val_accuracy: 0.7925\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0475 - accuracy: 0.9751 - val_loss: 0.7434 - val_accuracy: 0.7453\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0500 - accuracy: 0.9689 - val_loss: 1.1729 - val_accuracy: 0.6258\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0597 - accuracy: 0.9658 - val_loss: 1.1562 - val_accuracy: 0.6226\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0575 - accuracy: 0.9658 - val_loss: 0.9087 - val_accuracy: 0.7013\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0525 - accuracy: 0.9736 - val_loss: 0.7042 - val_accuracy: 0.7642\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0496 - accuracy: 0.9736 - val_loss: 0.6091 - val_accuracy: 0.7925\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0551 - accuracy: 0.9689 - val_loss: 0.6126 - val_accuracy: 0.7736\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0570 - accuracy: 0.9658 - val_loss: 0.4667 - val_accuracy: 0.8239\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0519 - accuracy: 0.9751 - val_loss: 0.4194 - val_accuracy: 0.8648\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0495 - accuracy: 0.9751 - val_loss: 0.4151 - val_accuracy: 0.8616\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0488 - accuracy: 0.9751 - val_loss: 0.4299 - val_accuracy: 0.8553\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0483 - accuracy: 0.9751 - val_loss: 0.4764 - val_accuracy: 0.8396\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0509 - accuracy: 0.9673 - val_loss: 0.6313 - val_accuracy: 0.7642\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0571 - accuracy: 0.9658 - val_loss: 0.6828 - val_accuracy: 0.7484\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0621 - accuracy: 0.9658 - val_loss: 0.6878 - val_accuracy: 0.7484\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0599 - accuracy: 0.9658 - val_loss: 0.5903 - val_accuracy: 0.7830\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0527 - accuracy: 0.9673 - val_loss: 0.5013 - val_accuracy: 0.8113\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0487 - accuracy: 0.9689 - val_loss: 0.4194 - val_accuracy: 0.8616\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0491 - accuracy: 0.9751 - val_loss: 0.3095 - val_accuracy: 0.8899\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0529 - accuracy: 0.9751 - val_loss: 0.2954 - val_accuracy: 0.8899\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0533 - accuracy: 0.9751 - val_loss: 0.3259 - val_accuracy: 0.8899\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0517 - accuracy: 0.9751 - val_loss: 0.3914 - val_accuracy: 0.8648\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0501 - accuracy: 0.9751 - val_loss: 0.4616 - val_accuracy: 0.8522\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0487 - accuracy: 0.9751 - val_loss: 0.5016 - val_accuracy: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0486 - accuracy: 0.9751 - val_loss: 0.5375 - val_accuracy: 0.8145\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0483 - accuracy: 0.9751 - val_loss: 0.5912 - val_accuracy: 0.7830\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0503 - accuracy: 0.9673 - val_loss: 0.7485 - val_accuracy: 0.7453\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0551 - accuracy: 0.9673 - val_loss: 0.7635 - val_accuracy: 0.7453\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0541 - accuracy: 0.9673 - val_loss: 0.6947 - val_accuracy: 0.7579\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0514 - accuracy: 0.9673 - val_loss: 0.6027 - val_accuracy: 0.7799\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0480 - accuracy: 0.9689 - val_loss: 0.5218 - val_accuracy: 0.8270\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0484 - accuracy: 0.9751 - val_loss: 0.3964 - val_accuracy: 0.8648\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0506 - accuracy: 0.9751 - val_loss: 0.3875 - val_accuracy: 0.8679\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0493 - accuracy: 0.9751 - val_loss: 0.4878 - val_accuracy: 0.8302\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0491 - accuracy: 0.9751 - val_loss: 0.5733 - val_accuracy: 0.8050\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0492 - accuracy: 0.9751 - val_loss: 0.6108 - val_accuracy: 0.7925\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0496 - accuracy: 0.9751 - val_loss: 0.6286 - val_accuracy: 0.7893\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0494 - accuracy: 0.9751 - val_loss: 0.6126 - val_accuracy: 0.7956\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0488 - accuracy: 0.9751 - val_loss: 0.5773 - val_accuracy: 0.8050\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0491 - accuracy: 0.9751 - val_loss: 0.4753 - val_accuracy: 0.8459\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0498 - accuracy: 0.9751 - val_loss: 0.4540 - val_accuracy: 0.8459\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0504 - accuracy: 0.9751 - val_loss: 0.4659 - val_accuracy: 0.8459\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0498 - accuracy: 0.9751 - val_loss: 0.4881 - val_accuracy: 0.8365\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0491 - accuracy: 0.9751 - val_loss: 0.5206 - val_accuracy: 0.8208\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0490 - accuracy: 0.9751 - val_loss: 0.5642 - val_accuracy: 0.8113\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0486 - accuracy: 0.9751 - val_loss: 0.5681 - val_accuracy: 0.8082\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0484 - accuracy: 0.9751 - val_loss: 0.4863 - val_accuracy: 0.8396\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0508 - accuracy: 0.9751 - val_loss: 0.4765 - val_accuracy: 0.8428\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0514 - accuracy: 0.9751 - val_loss: 0.5218 - val_accuracy: 0.8239\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0492 - accuracy: 0.9751 - val_loss: 0.5671 - val_accuracy: 0.8082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f7c985d448>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "nn2 = KerasClassifier(build_fn = build_network, \n",
    "                            epochs = 200,\n",
    "                            batch_size = 128)\n",
    "nn2.fit(X,y, validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions from fitted model\n",
    "predicts2 = nn2.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.919875\n",
      "Precision: 0.908437\n",
      "Recall: 0.940247\n",
      "F1 score: 0.921547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import precision_score \n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import cohen_kappa_score \n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import confusion_matrix  \n",
    "\n",
    "# accuracy: (tp + tn) / (p + n) \n",
    "accuracy = accuracy_score(y, predicts2) \n",
    "print('Accuracy: %f' % accuracy) \n",
    "\n",
    "# precision tp / (tp + fp) \n",
    "precision = precision_score(y, predicts2, average = 'macro') \n",
    "print('Precision: %f' % precision) \n",
    "\n",
    "# recall: tp / (tp + fn) \n",
    "recall = recall_score(y, predicts2, average = 'macro') \n",
    "print('Recall: %f' % recall) \n",
    "\n",
    "# f1: 2 tp / (2 tp + fp + fn) \n",
    "f1 = f1_score(y, predicts2, average = 'macro') \n",
    "print('F1 score: %f' % f1)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix \n",
    "matrix2 = confusion_matrix(y, predicts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 41   0   1]\n",
      " [  0 205  15]\n",
      " [  1  60 638]]\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "print(matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classifying Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set that the color channel value will be first\n",
    "#K.set_image_data_format(\"channels_first\")\n",
    "K.set_image_data_format(\"channels_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set image information\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and target from MNIST data\n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape training image data into features\n",
    "data_train = data_train.reshape(data_train.shape[0], height, width, channels)\n",
    "                                                 #  height x width x channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape test image data into features\n",
    "data_test = data_test.reshape(data_test.shape[0], height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale pixel intensity to between 0 and 1\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode target\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start neural network\n",
    "network = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add convolutional layer with 64 filters, a 5x5 window, and ReLU activation function\n",
    "network.add(Conv2D(filters = 64,\n",
    "                  kernel_size = (5, 5),\n",
    "                  input_shape=(width, height, channels),\n",
    "                  activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add max pooling layer with a 2x2 window\n",
    "network.add(MaxPooling2D(pool_size = (2, 2))) # , data_format='channels_last'\n",
    "#           MaxPooling2D(pool_size=[3, 3], strides=2, padding='same', data_format='channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add layer to flatten input\n",
    "network.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layer of 128 units with a ReLU activiation function\n",
    "network.add(Dense(128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layer with a softmax activiation function\n",
    "network.add(Dense(number_of_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile neural network\n",
    "network.compile(loss = \"categorical_crossentropy\", # Cross-entropy\n",
    "               optimizer = \"rmsprop\", # Root Mean Square Propagation\n",
    "               metrics = ['accuracy']) # Accuracy performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c681d20908>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train neural network\n",
    "network.fit(features_train, # Features\n",
    "           target_train, # Target\n",
    "           epochs = 2, # Number of epochs\n",
    "           verbose = 0, # Don't print description after each eposh\n",
    "           batch_size = 1000, # Number of observations per batch\n",
    "           validation_data = (features_test, target_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 64)        1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,182,730\n",
      "Trainable params: 1,182,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 87ms/step - loss: 0.0831 - accuracy: 0.9766\n",
      "\n",
      "Test accuracy: 97.7%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = network.evaluate(features_test, target_test, batch_size=1000)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** The model accuragy was 97.7 for this classification image.  This is based off the paramaters that are used within this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
